<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> TaiRO: AI for Contouring | Amith J. Kamath </title> <meta name="author" content="Amith J. Kamath"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://amithjkamath.github.io/blog/2025/04-TaiRO-Contouring/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Amith</span> J. Kamath </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Notes </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">more </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">Publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/teaching/">Teaching</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/glossary/">Glossary</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">TaiRO: AI for Contouring</h1> <p class="post-meta"> Created on April 01, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/category/tairo"> <i class="fa-solid fa-tag fa-sm"></i> tairo,</a>   <a href="/blog/category/radiation-oncology"> <i class="fa-solid fa-tag fa-sm"></i> radiation-oncology,</a>   <a href="/blog/category/artificial-intelligence"> <i class="fa-solid fa-tag fa-sm"></i> artificial-intelligence</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="4-ai-for-image-contouring">4: AI for Image Contouring</h1> <ul> <li> <a href="#4-ai-for-image-contouring">4: AI for Image Contouring</a> <ul> <li> <a href="#contouring-fundamentals">Contouring Fundamentals</a> <ul> <li><a href="#binary-vs-multi-class-segmentation">Binary vs. Multi-class Segmentation</a></li> <li><a href="#semantic-vs-instance-segmentation">Semantic vs. Instance Segmentation</a></li> </ul> </li> <li> <a href="#segmentation-architectures">Segmentation Architectures</a> <ul> <li><a href="#u-net-and-variants">U-Net and Variants</a></li> <li><a href="#v-net-for-3d-segmentation">V-Net for 3D Segmentation</a></li> <li><a href="#attention-based-segmentation">Attention-based Segmentation</a></li> </ul> </li> <li> <a href="#loss-functions-for-segmentation">Loss Functions for Segmentation</a> <ul> <li><a href="#dice-loss">Dice Loss</a></li> <li><a href="#focal-loss">Focal Loss</a></li> <li><a href="#boundary-aware-losses">Boundary-aware Losses</a></li> <li><a href="#combined-losses">Combined Losses</a></li> </ul> </li> <li> <a href="#evaluation-metrics-for-contouring">Evaluation Metrics for Contouring</a> <ul> <li><a href="#geometric-metrics">Geometric Metrics</a></li> <li><a href="#clinical-acceptability-measures">Clinical Acceptability Measures</a></li> <li><a href="#inter-observer-variability">Inter-observer Variability</a></li> </ul> </li> <li> <a href="#current-research-in-auto-contouring">Current Research in Auto-contouring</a> <ul> <li><a href="#organ-at-risk-oar-contouring">Organ-at-Risk (OAR) Contouring</a></li> <li><a href="#tumor-volume-delineation">Tumor Volume Delineation</a></li> <li><a href="#adaptive-contouring">Adaptive Contouring</a></li> <li><a href="#quality-assurance-and-clinical-integration">Quality Assurance and Clinical Integration</a></li> </ul> </li> <li><a href="#summary-and-references">Summary and References</a></li> </ul> </li> </ul> <p>Accurate delineation of target volumes and organs at risk (OARs) is a critical and often time-consuming step in radiation therapy planning. Manual contouring is subject to inter-observer variability and can be a significant bottleneck in the clinical workflow. Deep learning, particularly convolutional neural networks (CNNs), has shown remarkable success in automating this process, offering the potential for increased efficiency, consistency, and accuracy.</p> <h2 id="contouring-fundamentals">Contouring Fundamentals</h2> <p>Contouring in radiation oncology is fundamentally a segmentation task—assigning a label (e.g., tumor, specific OAR, background) to each voxel in a 3D medical image (typically CT or MRI). Understanding the different types of segmentation is essential for applying deep learning effectively.</p> <h3 id="binary-vs-multi-class-segmentation">Binary vs. Multi-class Segmentation</h3> <p>Binary segmentation involves distinguishing between two classes: the object of interest (foreground) and everything else (background). For example, segmenting a single OAR like the heart involves classifying each voxel as either “heart” or “not heart.”</p> <p>Multi-class segmentation extends this to multiple distinct classes simultaneously. In radiation oncology, this is common when contouring multiple OARs and potentially the target volume within the same image. The model must assign each voxel to one of several predefined classes (e.g., heart, lung, spinal cord, tumor, background).</p> <p>Deep learning models for segmentation typically output a probability map for each class. For binary segmentation, a single output channel with sigmoid activation provides the probability of belonging to the foreground class. For multi-class segmentation, multiple output channels with softmax activation provide probabilities for each class, ensuring they sum to one for each voxel.</p> <h3 id="semantic-vs-instance-segmentation">Semantic vs. Instance Segmentation</h3> <p>Semantic segmentation assigns a class label to each voxel but does not distinguish between different instances of the same class. For example, if multiple lymph nodes are present, semantic segmentation would label all of them as “lymph node” without differentiating individual nodes.</p> <p>Instance segmentation goes a step further by identifying and delineating each individual object instance. In the lymph node example, instance segmentation would provide a separate contour for each distinct node.</p> <p>While most contouring tasks in radiation oncology currently rely on semantic segmentation (delineating specific organs or tumor volumes), instance segmentation could be relevant for tasks like identifying individual metastatic lesions or tracking multiple objects over time.</p> <h2 id="segmentation-architectures">Segmentation Architectures</h2> <p>Several deep learning architectures have proven particularly effective for medical image segmentation, building upon the foundational concepts of CNNs and attention mechanisms discussed in Module 4.</p> <h3 id="u-net-and-variants">U-Net and Variants</h3> <ul> <li>U-Net</li> <li>Residual U-Net</li> <li>Attention U-Net</li> <li>U-Net++</li> </ul> <p>The U-Net architecture, specifically designed for biomedical image segmentation, remains the cornerstone of many contouring applications. Its encoder-decoder structure with skip connections effectively combines multi-scale feature extraction with precise spatial localization.</p> <p>Key features of U-Net include:</p> <p>Symmetric encoder-decoder paths: The encoder progressively reduces spatial resolution and increases feature channels, while the decoder symmetrically increases resolution and decreases channels.</p> <p>Skip connections: Concatenating feature maps from the encoder to corresponding layers in the decoder allows the network to reuse high-resolution features, crucial for accurate boundary delineation.</p> <p>Overlap-tile strategy (optional): For large images, U-Net can process overlapping patches and seamlessly combine the predictions.</p> <p>Numerous variants have built upon the U-Net foundation:</p> <p>Residual U-Net incorporates residual connections within the convolutional blocks, potentially improving gradient flow and enabling deeper networks.</p> <p>Attention U-Net integrates attention gates into the skip connections, allowing the model to selectively focus on relevant features being passed from the encoder to the decoder.</p> <p>U-Net++ uses nested and dense skip connections to further bridge the semantic gap between encoder and decoder features, potentially improving performance on complex segmentation tasks.</p> <p>These U-Net based architectures form the backbone of many automated contouring systems used or investigated in radiation oncology clinics.</p> <h3 id="v-net-for-3d-segmentation">V-Net for 3D Segmentation</h3> <p>While 2D U-Nets process images slice by slice, medical imaging data in radiation oncology is inherently 3D (CT, MRI volumes). V-Net extends the U-Net concept to fully 3D convolutions, allowing the network to directly leverage spatial context across slices.</p> <p>V-Net replaces 2D convolutions, pooling, and up-sampling operations with their 3D counterparts. It often incorporates residual connections within its convolutional blocks. By processing the entire 3D volume (or large 3D patches), V-Net can better capture the complex shapes and relationships of anatomical structures in three dimensions, potentially leading to more accurate and consistent segmentations compared to slice-by-slice 2D approaches.</p> <p>The main challenge with 3D architectures like V-Net is the significantly increased computational cost and memory requirements due to the cubic growth in data size. This often necessitates processing smaller 3D patches or using techniques like downsampling to manage resource constraints.</p> <h3 id="attention-based-segmentation">Attention-based Segmentation</h3> <p>Attention mechanisms, particularly self-attention as used in transformers, can enhance segmentation models by allowing them to capture long-range dependencies and focus on relevant image regions.</p> <p>Several architectures incorporate attention:</p> <p>Attention U-Net (mentioned earlier) uses attention gates in skip connections.</p> <p>Transformer-based segmentation models like UNETR and Swin UNETR replace or augment parts of the U-Net architecture (typically the encoder) with transformer blocks. These models can capture global context more effectively than pure CNN approaches, which can be beneficial for segmenting large or complex structures.</p> <p>Non-local networks incorporate self-attention modules within convolutional architectures to capture long-range spatial dependencies.</p> <p>Attention mechanisms can help models better understand the global context of an image, differentiate between similar-looking tissues based on surrounding structures, and improve segmentation accuracy, particularly for challenging cases with ambiguous boundaries or anatomical variations.</p> <h2 id="loss-functions-for-segmentation">Loss Functions for Segmentation</h2> <p>The choice of loss function significantly impacts how a segmentation model learns and the quality of the resulting contours. Standard classification losses like cross-entropy can be suboptimal for segmentation, especially with imbalanced data.</p> <ul> <li>Dice Loss</li> <li>Focal Loss</li> <li>Boundary-aware Losses</li> <li>Combined Losses</li> </ul> <h3 id="dice-loss">Dice Loss</h3> <p>Dice loss, derived from the Dice Similarity Coefficient (DSC), directly optimizes for overlap between the predicted and ground truth segmentations. It is defined as:</p> <p>L_Dice = 1 - DSC = 1 - (2 * ∑(p_i * g_i) + ε) / (∑p_i + ∑g_i + ε)</p> <p>Where p_i is the predicted probability for voxel i, g_i is the ground truth label (0 or 1), and ε is a small constant for numerical stability.</p> <p>Dice loss is particularly effective for imbalanced segmentation tasks because it focuses on the agreement between foreground predictions and ground truth, regardless of the number of background voxels. It has become a standard loss function for medical image segmentation.</p> <p>Variations like Generalized Dice Loss handle multi-class segmentation by weighting the contribution of each class based on its volume, preventing larger structures from dominating the loss.</p> <h3 id="focal-loss">Focal Loss</h3> <p>Focal loss, originally proposed for object detection, modifies the standard cross-entropy loss to down-weight the contribution of easy-to-classify examples (often the abundant background voxels) and focus training on harder examples (often foreground voxels or boundary regions):</p> <p>L_Focal = -α(1-p_t)^γ * log(p_t)</p> <p>Where p_t is the probability of the correct class, α balances class importance, and γ is the focusing parameter. Higher values of γ increase the focus on hard examples.</p> <p>Focal loss can be effective in segmentation tasks with extreme class imbalance, helping the model learn to correctly classify rare foreground structures.</p> <h3 id="boundary-aware-losses">Boundary-aware Losses</h3> <p>Accurate boundary delineation is often critical in radiation oncology. Boundary-aware losses explicitly penalize errors near the segmentation boundaries:</p> <p>Boundary Loss computes the discrepancy between the predicted boundary and the ground truth boundary, often using distance transforms.</p> <p>Weighted Cross-Entropy or Dice Loss can assign higher weights to voxels near the boundary, forcing the model to pay more attention to these critical regions.</p> <p>Shape-aware losses incorporate prior knowledge about the expected shape of the structure being segmented, penalizing predictions that deviate significantly from plausible shapes.</p> <h3 id="combined-losses">Combined Losses</h3> <p>In practice, combining multiple loss functions often yields the best results. Common combinations include:</p> <p>Dice + Cross-Entropy: Balances overlap-based optimization with pixel-wise classification accuracy.</p> <p>Dice + Focal Loss: Combines overlap optimization with focused learning on hard examples.</p> <p>Adding boundary loss terms to Dice or cross-entropy losses to specifically improve boundary accuracy.</p> <p>The optimal loss function or combination depends on the specific segmentation task, dataset characteristics, and clinical priorities for contouring accuracy.</p> <h2 id="evaluation-metrics-for-contouring">Evaluation Metrics for Contouring</h2> <p>Evaluating the performance of automated contouring models requires metrics that capture clinically relevant aspects of segmentation quality, going beyond simple overlap measures.</p> <h3 id="geometric-metrics">Geometric Metrics</h3> <p>Dice Similarity Coefficient (DSC) and Intersection over Union (IoU) remain standard metrics for assessing overall overlap.</p> <p>Hausdorff Distance (HD) measures the maximum distance between the surfaces of the predicted and ground truth contours, quantifying the largest boundary discrepancy. The 95th percentile HD (HD95) is often preferred as it is less sensitive to outliers.</p> <p>Average Symmetric Surface Distance (ASSD) calculates the average distance between the surfaces, providing a measure of overall boundary agreement.</p> <p>Volume Difference measures the percentage difference between the predicted and ground truth volumes, indicating whether the model tends to under- or overestimate the structure size.</p> <h3 id="clinical-acceptability-measures">Clinical Acceptability Measures</h3> <p>While geometric metrics provide quantitative measures, clinical acceptability often involves more nuanced assessment:</p> <p>Contour smoothness and regularity: Automated contours should ideally be smooth and anatomically plausible, without jagged edges or unrealistic shapes.</p> <p>Boundary adherence: The contour should accurately follow the visible boundary of the structure in the image.</p> <p>Inclusion of critical regions / Exclusion of nearby structures: The contour must reliably include the entire target or OAR while avoiding encroachment on adjacent critical structures.</p> <p>Rating scales or qualitative assessments by expert clinicians are often used alongside geometric metrics to evaluate the clinical usability of automated contours. Studies may measure the percentage of automatically generated contours that require no edits, minor edits, or major edits by a clinician.</p> <h3 id="inter-observer-variability">Inter-observer Variability</h3> <p>It is crucial to compare the performance of automated contouring models against the inherent variability observed between human experts. A model that achieves performance comparable to inter-observer agreement is often considered clinically acceptable.</p> <p>Metrics like DSC, HD, and ASSD can be calculated between contours drawn by different clinicians on the same images to establish a baseline for human variability. The automated model’s performance is then compared to this baseline.</p> <p>Evaluating against multiple expert delineations (e.g., using the STAPLE algorithm to estimate a consensus ground truth) provides a more robust assessment than comparing against a single expert contour.</p> <h2 id="current-research-in-auto-contouring">Current Research in Auto-contouring</h2> <p>Research in deep learning for auto-contouring continues to advance rapidly, addressing remaining challenges and expanding capabilities.</p> <h3 id="organ-at-risk-oar-contouring">Organ-at-Risk (OAR) Contouring</h3> <p>Automated contouring of OARs is one of the most mature applications of deep learning in radiation oncology. Models have demonstrated high accuracy for many common OARs across different anatomical sites (head and neck, thorax, abdomen, pelvis).</p> <p>Current research focuses on:</p> <p>Improving robustness across different imaging protocols, scanners, and patient populations.</p> <p>Handling challenging OARs with low contrast or ambiguous boundaries.</p> <p>Developing models that can segment a comprehensive set of OARs simultaneously.</p> <p>Integrating uncertainty estimation to flag contours that may require manual review.</p> <p>Validating performance in large-scale, multi-institutional studies.</p> <h3 id="tumor-volume-delineation">Tumor Volume Delineation</h3> <p>Segmenting the gross tumor volume (GTV) and clinical target volume (CTV) is often more challenging than OAR contouring due to the variability in tumor appearance, infiltration patterns, and reliance on multi-modal imaging (e.g., PET-CT, MRI).</p> <p>Research efforts include:</p> <p>Developing multi-modal segmentation models that effectively fuse information from different imaging sources.</p> <p>Incorporating prior knowledge about tumor growth patterns or anatomical constraints.</p> <p>Addressing the significant inter-observer variability in tumor delineation.</p> <p>Predicting microscopic tumor extension for CTV definition.</p> <p>Using deep learning to identify tumor subregions with different biological characteristics (e.g., hypoxia, proliferation) based on imaging features (radiomics).</p> <h3 id="adaptive-contouring">Adaptive Contouring</h3> <p>During a course of radiation therapy, patient anatomy can change due to tumor shrinkage, weight loss, or organ motion. Adaptive radiotherapy requires re-contouring on images acquired during treatment (e.g., cone-beam CT).</p> <p>Deep learning models are being developed for:</p> <p>Deformable image registration to propagate initial contours to subsequent images.</p> <p>Direct segmentation on daily or weekly images, potentially adapting to changes over time.</p> <p>Predicting future anatomical changes to enable proactive plan adaptation.</p> <p>Challenges include the lower image quality of cone-beam CT compared to planning CT and the need for rapid processing to enable online adaptation.</p> <h3 id="quality-assurance-and-clinical-integration">Quality Assurance and Clinical Integration</h3> <p>A critical area of research involves developing methods for quality assurance (QA) of automated contours and facilitating safe clinical integration:</p> <p>Developing AI-based QA tools that can automatically flag potentially erroneous contours for human review.</p> <p>Quantifying the impact of auto-contouring on downstream treatment planning and predicted outcomes.</p> <p>Designing optimal workflows that combine automated contouring with efficient human review and editing.</p> <p>Establishing best practices for commissioning, validating, and monitoring auto-contouring systems in clinical practice.</p> <p>As deep learning models for auto-contouring continue to improve in accuracy and robustness, their integration into clinical workflows holds the potential to significantly enhance the efficiency and consistency of radiation therapy planning.</p> <h2 id="summary-and-references">Summary and References</h2> <p>This article provides an overview of the application of deep learning, particularly convolutional neural networks and attention-based models, for automating the contouring process in radiation therapy planning. It explains the fundamentals of medical image segmentation, discusses key architectures like U-Net, V-Net, and transformer-based models, and reviews various loss functions and evaluation metrics relevant to clinical practice. The article also highlights current research directions, including organ-at-risk and tumor volume segmentation, adaptive contouring, and quality assurance, emphasizing the challenges and advancements in integrating AI-driven auto-contouring into clinical workflows to improve efficiency, consistency, and accuracy in radiation oncology.</p> <ul> <li><a href="https://arxiv.org/abs/1505.04597" rel="external nofollow noopener" target="_blank">U-Net Paper</a></li> <li><a href="https://arxiv.org/abs/1606.04797" rel="external nofollow noopener" target="_blank">V-Net Paper</a></li> <li>…</li> </ul> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://www.caim.unibe.ch/about_us/people/interviews/amith_kamath/index_eng.html" target="_blank" rel="external nofollow noopener">About Us: "I would like to convert my research into a useful tool for clinicians." - Center for Artificial Intelligence in Medicine (CAIM)</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/07-TaiRO-Treatment-Planning/">TaiRO: AI for Treatment Planning</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/03-TaiRO-Imaging/">Medical Imaging Modalities and Storage File Types in Radiation Oncology</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/02-TaiRO-Fundamentals/">TaiRO: Fundamentals of Deep Learning</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/autocontour-need/">Market need for Auto-Contouring Solutions</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Amith J. Kamath. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: May 28, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>