---
layout: post
title: 'TaiRO: Introduction'
date: 2025-01-01 00:00:00
categories: tairo, radiation-oncology, artificial-intelligence
featured: true
---

# 1: Introduction

## Historical Context and Evolution of AI

Artificial Intelligence as a field has roots dating back to the 1950s, but the concepts that would eventually lead to modern AI began much earlier with philosophical questions about the nature of knowledge, reasoning, and the possibility of creating thinking machines. The formal birth of AI as a discipline is often attributed to the Dartmouth Conference of 1956, where John McCarthy, Marvin Minsky, Claude Shannon, and others gathered to discuss the possibility of creating machines that could "think."

The early decades of AI research were characterized by periods of great optimism followed by "AI winters" when progress slowed and funding decreased. Early AI approaches focused on symbolic reasoning and rule-based systems, attempting to encode human knowledge explicitly. These systems showed promise in narrow domains but struggled with the complexity and ambiguity of real-world problems.

The evolution of AI has been marked by several paradigm shifts. From the rule-based expert systems of the 1970s and 1980s to the statistical approaches that gained prominence in the 1990s, each era brought new insights and techniques. The current deep learning revolution, which began in earnest around 2012 with breakthroughs in image recognition using convolutional neural networks, represents perhaps the most significant shift yet.

In radiation oncology, this evolution mirrors the broader field, with early applications focusing on rule-based planning systems, followed by statistical models for outcome prediction, and now deep learning approaches for tasks ranging from image segmentation to treatment planning optimization.

## Types of Machine Learning

Machine learning, a subset of AI, focuses on developing algorithms that can learn patterns from data without being explicitly programmed. The field is typically divided into several learning paradigms:

### Supervised Learning

Supervised learning involves training models on labeled data, where each input is paired with the desired output. The algorithm learns to map inputs to outputs by minimizing the difference between its predictions and the ground truth labels. This approach is particularly relevant in radiation oncology for tasks like tumor classification, where historical images with confirmed diagnoses serve as training data.

In supervised learning, the quality and quantity of labeled data are crucial factors in model performance. For medical applications, obtaining high-quality labeled data often requires expert annotation, which can be time-consuming and expensive. This challenge is particularly acute in radiation oncology, where inter-observer variability in contouring can introduce inconsistencies in training data.

Common supervised learning tasks include classification (assigning inputs to discrete categories) and regression (predicting continuous values). In radiation oncology, classification might involve determining whether tissue is cancerous, while regression could predict radiation dose distribution.

### Unsupervised Learning

Unsupervised learning works with unlabeled data, seeking to discover inherent patterns or structures. Without explicit guidance on what constitutes a "correct" output, these algorithms identify natural groupings, reduce dimensionality, or detect anomalies in data.

Clustering algorithms, a major category of unsupervised learning, group similar data points together based on distance metrics. In radiation oncology, clustering might be used to identify patient subgroups with similar treatment responses or to detect patterns in treatment planning that correlate with outcomes.

Dimensionality reduction techniques like Principal Component Analysis (PCA) or t-SNE help visualize high-dimensional data and can reveal underlying structures not immediately apparent in the original feature space. These techniques can be valuable for analyzing the complex, multi-dimensional data generated in radiation treatment planning.

Anomaly detection algorithms identify outliers or unusual patterns in data, which could represent equipment malfunctions, unusual patient anatomy, or potential errors in treatment plans.

### Reinforcement Learning

Reinforcement learning involves training agents to make sequences of decisions by rewarding desired behaviors and penalizing undesired ones. Unlike supervised learning, there are no labeled examples; instead, the agent learns through trial and error, guided by a reward signal.

This paradigm has shown promise in treatment planning optimization, where the algorithm can learn to generate plans that maximize tumor coverage while minimizing dose to organs at risk. The reward function in such applications might incorporate clinical objectives like dose constraints and target coverage metrics.

Reinforcement learning faces challenges in medical applications due to the need for extensive exploration (trying different actions to learn their outcomes), which may not be feasible in clinical settings where patient safety is paramount. Simulation environments and digital twins offer potential solutions, allowing algorithms to learn in virtual environments before deployment in clinical practice.

### Semi-supervised and Self-supervised Learning

Semi-supervised learning combines elements of supervised and unsupervised learning, using a small amount of labeled data alongside a larger pool of unlabeled data. This approach is particularly relevant in medical imaging, where expert annotations may be limited but unannotated images are abundant.

Self-supervised learning, a growing area of research, involves creating supervised learning tasks from unlabeled data by generating labels automatically. For example, an algorithm might be trained to predict missing portions of an image, with the complete image serving as the ground truth. These approaches show promise for pre-training models when labeled data is scarce.

## Key Machine Learning Algorithms

Before diving into deep learning, it's essential to understand the traditional machine learning algorithms that form the foundation of the field. These algorithms continue to be valuable tools, especially when data is limited or interpretability is crucial.

### Linear and Logistic Regression

Linear regression, one of the simplest machine learning algorithms, models the relationship between input features and a continuous output variable as a linear function. Despite its simplicity, linear regression provides a foundation for understanding more complex models and can be surprisingly effective for certain problems.

Logistic regression extends this concept to classification problems by applying a sigmoid function to the linear output, transforming it into a probability between 0 and 1. This algorithm has been used in radiation oncology for predicting binary outcomes like tumor control or the development of specific toxicities.

Both linear and logistic regression offer high interpretability, as the contribution of each feature to the prediction is explicitly represented by its coefficient. This transparency is valuable in clinical settings where understanding the basis for predictions is essential.

### Decision Trees and Random Forests

Decision trees partition the feature space through a series of binary splits, creating a tree-like structure where each leaf node represents a prediction. These models are intuitive and can capture non-linear relationships, but individual trees are prone to overfitting.

Random forests address this limitation by combining many decision trees trained on different subsets of the data and features. The ensemble prediction is typically more robust and accurate than any individual tree. In radiation oncology, random forests have been used for tasks like predicting patient outcomes based on clinical factors, dosimetric parameters, and imaging features.

Tree-based methods offer several advantages for medical applications, including handling mixed data types, robustness to outliers, and the ability to capture complex interactions between features. They also provide measures of feature importance, helping identify the most relevant factors for a given prediction task.

### Support Vector Machines

Support Vector Machines (SVMs) find the optimal hyperplane that separates different classes in the feature space, maximizing the margin between the closest points (support vectors) from each class. Through the use of kernel functions, SVMs can efficiently handle non-linear decision boundaries.

SVMs have been applied in radiation oncology for tasks like classifying treatment outcomes based on dosimetric and clinical features. Their ability to work well with relatively small datasets makes them suitable for many medical applications where data may be limited.

### K-means Clustering

K-means clustering, an unsupervised learning algorithm, partitions data into K clusters by iteratively assigning points to the nearest cluster center and then updating those centers. This algorithm can identify natural groupings in patient data, potentially revealing subpopulations with distinct characteristics or treatment responses.

In radiation oncology, k-means clustering has been used to identify patient subgroups based on anatomical features, dose distributions, or treatment outcomes. These insights can inform personalized treatment approaches and help identify patients who might benefit from alternative strategies.

## Feature Engineering and Selection

Feature engineering—the process of creating, transforming, and selecting relevant features from raw data—plays a crucial role in the success of traditional machine learning algorithms. While deep learning can automatically learn useful representations from raw data, feature engineering remains important for many applications, especially when working with structured data or when interpretability is a priority.

Common feature engineering techniques include:

1. **Normalization and standardization**: Scaling features to a common range or distribution to prevent certain features from dominating the learning process due to their magnitude.

2. **Polynomial features**: Creating interaction terms between existing features to capture non-linear relationships.

3. **Discretization**: Converting continuous variables into categorical ones, which can sometimes reveal patterns not apparent in the continuous representation.

4. **Text and image processing**: Extracting meaningful features from unstructured data like medical reports or images.

Feature selection helps identify the most informative features while reducing dimensionality, which can improve model performance, reduce overfitting, and enhance interpretability. Methods include filter approaches (selecting features based on statistical measures), wrapper methods (evaluating feature subsets based on model performance), and embedded methods (incorporating feature selection into the model training process).

In radiation oncology, domain knowledge plays a vital role in feature engineering. Clinically relevant features might include dosimetric parameters (like V20 for lung or mean heart dose), anatomical measurements, or derived metrics that capture aspects of the dose distribution known to correlate with outcomes.

## Model Evaluation Basics

Proper evaluation is essential for understanding model performance and making informed decisions about deployment. Several key concepts underpin effective model evaluation:

### Train-Test Splits and Cross-Validation

Splitting data into training and testing sets allows evaluation on unseen data, providing a more realistic assessment of how the model will perform in practice. Cross-validation extends this concept by performing multiple train-test splits and averaging the results, providing a more robust performance estimate.

In medical applications with limited data, techniques like stratified k-fold cross-validation help ensure that important subgroups are represented in both training and testing sets. Leave-one-out cross-validation, where each sample serves as the test set once, can be appropriate for very small datasets.

### Overfitting and Underfitting

Overfitting occurs when a model learns the training data too well, capturing noise rather than the underlying pattern. This results in poor generalization to new data. Underfitting, conversely, happens when a model is too simple to capture the underlying pattern.

Regularization techniques, early stopping, and proper validation can help detect and mitigate overfitting. In radiation oncology, where datasets are often small, the risk of overfitting is particularly high, making these techniques especially important.

### Performance Metrics

Different metrics capture different aspects of model performance:

- **Accuracy**: The proportion of correct predictions, useful for balanced classification problems.
- **Precision and Recall**: Important for imbalanced problems, precision measures the proportion of positive predictions that are correct, while recall measures the proportion of actual positives that are correctly identified.
- **F1 Score**: The harmonic mean of precision and recall, providing a balance between the two.
- **Area Under the ROC Curve (AUC)**: Measures the model's ability to discriminate between classes across different threshold settings.
- **Mean Squared Error (MSE) and Mean Absolute Error (MAE)**: Common metrics for regression problems.

In radiation oncology, domain-specific metrics are often more relevant than generic ones. For contouring tasks, metrics like Dice similarity coefficient and Hausdorff distance measure the overlap and maximum distance between predicted and ground truth contours. For treatment planning, metrics might include target coverage, conformity indices, and dose to organs at risk.

## Limitations of Traditional Machine Learning

While traditional machine learning algorithms have proven valuable in many applications, they have several limitations that deep learning addresses:

1. **Feature engineering dependency**: Traditional algorithms rely heavily on manual feature engineering, which requires domain expertise and can miss complex patterns that aren't explicitly encoded.

2. **Difficulty with unstructured data**: Images, text, and other unstructured data types are challenging for traditional algorithms without extensive preprocessing.

3. **Limited representation capacity**: Many traditional algorithms struggle to capture complex, hierarchical patterns in data.

4. **Fixed model complexity**: The complexity of traditional models is often fixed or limited by design, constraining their ability to scale with data size.

5. **Separate learning stages**: Traditional pipelines often involve separate stages for feature extraction and model training, preventing end-to-end optimization.

Deep learning addresses these limitations through its ability to automatically learn hierarchical representations from raw data, scale with data and computational resources, and enable end-to-end training. However, traditional methods retain advantages in scenarios with limited data, when interpretability is crucial, or when computational resources are constrained.

Understanding these traditional approaches provides important context for appreciating the innovations and capabilities of deep learning, which we'll explore in subsequent modules. It also helps identify situations where simpler models might be more appropriate than complex deep learning architectures.

### Further Reading

"Artificial intelligence in radiation oncology": [This article ](https://pubmed.ncbi.nlm.nih.gov/32843739/) provides a comprehensive overview of AI methods and their implications in the radiation therapy process. ​

"Revolutionizing radiation therapy: the role of AI in clinical practice": [This article](https://academic.oup.com/jrr/article/65/1/1/7441099) discusses how AI has optimized tumor and organ segmentation, saving time for radiation oncologists. ​

"Artificial intelligence in radiation oncology: A review of its current applications and future outlook": [This review](https://www.sciencedirect.com/science/article/pii/S1078817421000924) explores the potential of AI in toxicity prediction, automated treatment planning, and clinical trial patient selection. 

"Artificial intelligence and machine learning in cancer imaging": [This article](https://www.nature.com/articles/s43856-022-00199-0) discusses the challenges and opportunities of AI and ML in cancer imaging, considerations for the development of algorithms into tools that can be widely used and disseminated, and the development of the ecosystem needed to promote growth of AI and ML in cancer imaging.