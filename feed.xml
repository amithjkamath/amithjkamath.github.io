<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://amithjkamath.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://amithjkamath.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-05-22T10:43:29+00:00</updated><id>https://amithjkamath.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">TaiRO: Medical Imaging in Radiation Oncology</title><link href="https://amithjkamath.github.io/blog/2025/03-TaiRO-Imaging/" rel="alternate" type="text/html" title="TaiRO: Medical Imaging in Radiation Oncology"/><published>2025-03-01T00:00:00+00:00</published><updated>2025-03-01T00:00:00+00:00</updated><id>https://amithjkamath.github.io/blog/2025/03-TaiRO-Imaging</id><content type="html" xml:base="https://amithjkamath.github.io/blog/2025/03-TaiRO-Imaging/"><![CDATA[<table> <thead> <tr> <th>Modality</th> <th>Typical Uses</th> <th>Duration</th> <th>Pros</th> <th>Cons</th> </tr> </thead> <tbody> <tr> <td>X-ray</td> <td>Fracture diagnosis; Lung infection detection; Dental evaluation</td> <td>10–15 minutes</td> <td>Quick and accessible; Relatively low cost; Effective for detecting fractures and lung conditions</td> <td>Limited soft tissue detail; Exposure to ionizing radiation</td> </tr> <tr> <td>Fluoroscopy</td> <td>Barium enema procedures; Cardiac catheterization; Joint injections</td> <td>30 minutes – 2 hours</td> <td>Real-time imaging; Guidance during procedures</td> <td>Exposure to ionizing radiation; Potential for contrast dye reactions</td> </tr> <tr> <td>CT Scan</td> <td>Tumor detection and staging; Vascular disease evaluation; Internal injury assessment</td> <td>20–25 minutes</td> <td>High-resolution images; Fast scanning times; Excellent for bone and vascular evaluation</td> <td>Higher radiation dose than X-rays; Contrast dye may be required</td> </tr> <tr> <td>MRI</td> <td>Brain and spinal cord imaging; Soft tissue evaluation; Multiple sclerosis diagnosis</td> <td>45 minutes – 1 hour</td> <td>Detailed soft tissue images; No ionizing radiation; Multiplanar imaging capabilities</td> <td>Longer scanning times; Claustrophobic for some patients; Limited availability for certain conditions</td> </tr> <tr> <td>Ultrasound</td> <td>Prenatal imaging and monitoring; Abdominal and pelvic evaluation; Cardiac and vascular imaging</td> <td>30 minutes – 1 hour</td> <td>Real-time imaging; No ionizing radiation; Safe for pregnant women</td> <td>Operator-dependent; Limited penetration for deep structures</td> </tr> <tr> <td>PET Scan</td> <td>Cancer diagnosis and staging; Brain function evaluation; Heart disease assessment</td> <td>1.5 – 2 hours</td> <td>Functional and metabolic information; Detection of small lesions; Accurate staging of cancers</td> <td>High cost; Limited availability; Requires radiotracer administration</td> </tr> <tr> <td>Mammography</td> <td>Breast cancer screening; Detection of breast abnormalities</td> <td>30 minutes</td> <td>Early detection of breast cancer; High-resolution images</td> <td>Slight discomfort during the procedure</td> </tr> </tbody> </table> <ol> <li> <p>X-Ray How it works: Uses ionizing radiation to produce 2D images of the body. Use cases: Bone fractures, lung infections (like pneumonia), chest imaging, dental exams. Speed: Very fast (few minutes). Cost: Low. Radiation: Low dose. Limitations: Poor soft tissue contrast; overlapping structures can obscure details.</p> </li> <li> <p>CT (Computed Tomography) How it works: Combines X-ray images taken from different angles into cross-sectional views. Use cases: Trauma, cancer, stroke, internal bleeding. Speed: Fast (minutes). Cost: Moderate to high. Radiation: Higher than X-ray (can be 100–1,000 times more). Strengths: Great for bone, chest, and detecting bleeding. Limitations: High radiation; contrast dye may affect kidneys.</p> </li> <li> <p>MRI (Magnetic Resonance Imaging) How it works: Uses magnets and radio waves to produce detailed images of soft tissues. Use cases: Brain, spinal cord, muscles, joints, tumors. Speed: Slower (30–90 minutes). Cost: High. Radiation: None. Strengths: Best soft tissue contrast; no radiation. Limitations: Claustrophobia, loud, not suitable with metal implants.</p> </li> <li> <p>Ultrasound How it works: High-frequency sound waves create real-time images. Use cases: Pregnancy, abdomen, heart (echocardiogram), blood flow. Speed: Real-time (immediate). Cost: Low to moderate. Radiation: None. Strengths: Safe in pregnancy, portable, real-time imaging. Limitations: Operator-dependent; limited by gas or obesity.</p> </li> <li> <p>Nuclear Medicine (e.g., PET, SPECT) How it works: Injects small amounts of radioactive material to assess function (not just structure). Use cases: Cancer detection, heart disease, brain disorders. Speed: Variable (can take hours). Cost: High. Radiation: Moderate to high (depends on tracer). Strengths: Functional imaging; early disease detection. Limitations: Radiation exposure; long prep and scan times.</p> </li> </ol> <p>“How tomographic reconstruction works?”: <a href="https://www.youtube.com/watch?v=f0sxjhGHRPo">This video</a> inspired by 3Blue1Brown shows how CT images are reconstructed in 3D using a series of single plane projections.</p> <p>“Radiology Modalities Explained: Understanding Medical Imaging Techniques”: <a href="https://ccdcare.com/resource-center/radiology-modalities/">This article</a> includes an overview of various radiology modalities, including X-rays, CT scans, MRI, ultrasound, and nuclear medicine. It explains how each modality works, their diagnostic applications, and considerations regarding radiation exposure.</p>]]></content><author><name></name></author><category term="tairo,"/><category term="radiation-oncology,"/><category term="artificial-intelligence"/><summary type="html"><![CDATA[Modality Typical Uses Duration Pros Cons X-ray Fracture diagnosis; Lung infection detection; Dental evaluation 10–15 minutes Quick and accessible; Relatively low cost; Effective for detecting fractures and lung conditions Limited soft tissue detail; Exposure to ionizing radiation Fluoroscopy Barium enema procedures; Cardiac catheterization; Joint injections 30 minutes – 2 hours Real-time imaging; Guidance during procedures Exposure to ionizing radiation; Potential for contrast dye reactions CT Scan Tumor detection and staging; Vascular disease evaluation; Internal injury assessment 20–25 minutes High-resolution images; Fast scanning times; Excellent for bone and vascular evaluation Higher radiation dose than X-rays; Contrast dye may be required MRI Brain and spinal cord imaging; Soft tissue evaluation; Multiple sclerosis diagnosis 45 minutes – 1 hour Detailed soft tissue images; No ionizing radiation; Multiplanar imaging capabilities Longer scanning times; Claustrophobic for some patients; Limited availability for certain conditions Ultrasound Prenatal imaging and monitoring; Abdominal and pelvic evaluation; Cardiac and vascular imaging 30 minutes – 1 hour Real-time imaging; No ionizing radiation; Safe for pregnant women Operator-dependent; Limited penetration for deep structures PET Scan Cancer diagnosis and staging; Brain function evaluation; Heart disease assessment 1.5 – 2 hours Functional and metabolic information; Detection of small lesions; Accurate staging of cancers High cost; Limited availability; Requires radiotracer administration Mammography Breast cancer screening; Detection of breast abnormalities 30 minutes Early detection of breast cancer; High-resolution images Slight discomfort during the procedure]]></summary></entry><entry><title type="html">TaiRO: Fundamentals of Machine Learning and Deep Learning</title><link href="https://amithjkamath.github.io/blog/2025/02-TaiRO-Fundamentals/" rel="alternate" type="text/html" title="TaiRO: Fundamentals of Machine Learning and Deep Learning"/><published>2025-02-01T00:00:00+00:00</published><updated>2025-02-01T00:00:00+00:00</updated><id>https://amithjkamath.github.io/blog/2025/02-TaiRO-Fundamentals</id><content type="html" xml:base="https://amithjkamath.github.io/blog/2025/02-TaiRO-Fundamentals/"><![CDATA[<p><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC5959832">Hello World: Deep Learning in Medical Imaging</a> is a dated but useful starting point for medical image classification using a standard off-the-shelf deep neural network architecture.</p>]]></content><author><name></name></author><category term="tairo,"/><category term="radiation-oncology,"/><category term="artificial-intelligence"/><summary type="html"><![CDATA[Hello World: Deep Learning in Medical Imaging is a dated but useful starting point for medical image classification using a standard off-the-shelf deep neural network architecture.]]></summary></entry><entry><title type="html">TaiRO: Introduction</title><link href="https://amithjkamath.github.io/blog/2025/01-TaiRO-Intro/" rel="alternate" type="text/html" title="TaiRO: Introduction"/><published>2025-01-01T00:00:00+00:00</published><updated>2025-01-01T00:00:00+00:00</updated><id>https://amithjkamath.github.io/blog/2025/01-TaiRO-Intro</id><content type="html" xml:base="https://amithjkamath.github.io/blog/2025/01-TaiRO-Intro/"><![CDATA[<p>“Artificial intelligence in radiation oncology”: <a href="https://pubmed.ncbi.nlm.nih.gov/32843739/">This article </a> provides a comprehensive overview of AI methods and their implications in the radiation therapy process. ​</p> <p>“Revolutionizing radiation therapy: the role of AI in clinical practice”: <a href="https://academic.oup.com/jrr/article/65/1/1/7441099">This article</a> discusses how AI has optimized tumor and organ segmentation, saving time for radiation oncologists. ​</p> <p>“Artificial intelligence in radiation oncology: A review of its current applications and future outlook”: <a href="https://www.sciencedirect.com/science/article/pii/S1078817421000924">This review</a> explores the potential of AI in toxicity prediction, automated treatment planning, and clinical trial patient selection.</p> <p>“Artificial intelligence and machine learning in cancer imaging”: <a href="https://www.nature.com/articles/s43856-022-00199-0">This article</a> discusses the challenges and opportunities of AI and ML in cancer imaging, considerations for the development of algorithms into tools that can be widely used and disseminated, and the development of the ecosystem needed to promote growth of AI and ML in cancer imaging.</p>]]></content><author><name></name></author><category term="tairo,"/><category term="radiation-oncology,"/><category term="artificial-intelligence"/><summary type="html"><![CDATA[“Artificial intelligence in radiation oncology”: This article provides a comprehensive overview of AI methods and their implications in the radiation therapy process. ​]]></summary></entry><entry><title type="html">Paper Summary: Deep learning in medical imaging and radiation therapy</title><link href="https://amithjkamath.github.io/blog/2024/deep-learning-in-radiation-therapy/" rel="alternate" type="text/html" title="Paper Summary: Deep learning in medical imaging and radiation therapy"/><published>2024-01-01T00:00:00+00:00</published><updated>2024-01-01T00:00:00+00:00</updated><id>https://amithjkamath.github.io/blog/2024/deep-learning-in-radiation-therapy</id><content type="html" xml:base="https://amithjkamath.github.io/blog/2024/deep-learning-in-radiation-therapy/"><![CDATA[<h2 id="background-and-introduction">Background and Introduction</h2> <p>The success of DL compared to traditional machine learning methods is primarily based on two interrelated factors: depth and compositionality. A function is said to have a compact expression if it has few computational elements used to represent it (“few” here is a relative term that depends on the complexity of the function). An architecture with sufficient depth can produce a compact representation, whereas an insufficiently deep one may require an exponentially larger architecture (in terms of the number of computational elements that need to be learned) to represent the same function.</p> <p>A compact representation requires fewer training examples to tune the parameters and produces better generalization to unseen examples. This is critically important in complex tasks such as computer vision where each object class can exhibit many variations in appearance which would potentially require several examples per type of variation in the training set if a compact representation is not used.</p> <p>The second advantage of deep architectures has to do with how successive layers of the network can utilize the representations from previous layers to compose more complex representations that better capture critical characteristics of the input data and suppress the irrelevant variations (for instance, simple translations of an object in the image should result in the same classification). In image recognition, deep networks have been shown to capture simple information such as the presence or absence of edges at different locations and orientations in the first layer. Successive layers of the network assemble the edges into compound edges and corners of shapes, and then into more and more complex shapes that resemble object parts.</p> <p>Hierarchical representation learning is very useful in complicated tasks such as computer vision where adjacent pixels and object parts are correlated with each other and their relative locations provide clues about each class of object, or speech recognition and natural language processing where the sequence of words follow contextual and grammatical rules that can be learned from the data.</p> <h2 id="convolutional-neural-networks">Convolutional Neural Networks</h2> <p>The most successful and popular DL architecture in imaging is the convolutional neural network (CNN). Nearby pixels in an image are correlated with one another both in areas that exhibit local smoothness and areas consisting of structures (e.g., edges of objects or textured regions). These correlations typically manifest themselves in different parts of the same image. Accordingly, instead of having a fully connected network where every pixel is processed by a different weight, every location can be processed using the same set of weights to extract various repeating patterns across the entire image. These sets of trainable weights, referred to as kernels or filters, are applied to the image using a dot product or convolution and then processed by a nonlinearity (e.g., a sigmoid or tanh function). Each of these convolution layers can consist of many such filters resulting in the extraction of multiple sets of patterns at each layer.</p> <h2 id="as-applied-to-medical-imaging">As applied to Medical Imaging</h2> <p>In medical imaging, machine learning algorithms have been used for decades, starting with algorithms to analyze or help interpret radiographic images in the mid-1960s. Computer-aided detection/diagnosis (CAD) algorithms started to make advances in the mid 1980s, first with algorithms dedicated to cancer detection and diagnosis on chest radiographs and mammograms and then widening in scope to other modalities such as computed tomography (CT) and ultrasound. CAD algorithms in the early days predominantly used a data-driven approach as most DL algorithms do today. However, unlike most DL algorithms, most of these early CAD methods heavily depended on feature engineering.</p> <p>DL for radiological images, and shows a very strong trend: For example, in the first 3 months of 2018, more papers were published on this topic than the whole year of 2016.</p> <h2 id="image-segmentation-with-deep-learning">Image Segmentation with Deep Learning</h2> <p>Image segmentation in medical imaging based on DL generally uses two different input methods: (a) patches of an input image and (b) the entire image. Both methods generate an output map that provides the likelihood that a given region is part of the object being segmented. While patch-based segmentation methods were initially used, most recent studies use the entire input image to give contextual information and reduce redundant calculations.</p> <p>Lesion segmentation is a similar task to organ segmentation; however, lesion segmentation is generally more difficult than organ segmentation, as the object being segmented can have varying shapes and sizes.</p> <h2 id="dl-and-radiotherapy">DL and Radiotherapy</h2> <p>The goals of DL in radiation oncology are to assist in treatment planning, assess response to therapy, and provide automated adaptation in treatments over time. Deep reinforcement learning using both prior treatment plans and methods for assessing tumor local control was used to automatically estimate dose protocols. Such adaptive radiotherapy methods may provide clinical decision support for dose adaptation.</p> <p>Much of the needs in treatment planning relate to the segmentation of organs (discussed earlier) and in the prediction of dose distributions from contours. Nguyen et al used a U-net to predict dose from patient image contours on prostate intensity-modulated radiation therapy (IMRT) patients and demonstrated desired radiation dose distributions.</p> <p>While DL methods are being developed to plan and predict radiation therapy to specific tumor sites, they are also being investigated to assess toxicity to normal organs tissue.</p> <h2 id="more-to-read-from-here">More to read from here:</h2> <p>281: Zhen et al. used a transfer learning strategy to predict rectum dose toxicity for cervical cancer radiotherapy.</p> <p>283: Dose estimation was also the aim of Kajikawa et al. who investigated the feasibility of DL in the automated determination of dosimetric eligibility of prostate cancer patients undergoing intensity-modulated radiation therapy.</p> <p>218: Lao et al. investigated MRI radiomic features and DL as a means to predict survival in glioblastoma multiforme.</p> <h2 id="challenges-for-deep-learning-methods">Challenges for Deep Learning methods</h2> <p>Robustness is a challenge: Robustness and repeatability are concerns with any machine learning approach,365 and even more so with DL. Since medical image datasets are so difficult to come by compared to those of natural images and generally are of limited size, researchers like to reuse the same data for different tasks. Hence, correction for multiple comparisons is crucial in the statistical evaluation of performance. The requirement that datasets need to be of sufficient size and quality is not unique to DL or medical imaging.</p> <h2 id="references">References</h2> <p>Sahiner, B., Pezeshk, A., Hadjiiski, L. M., Wang, X., Drukker, K., Cha, K. H., Summers, R. M., &amp; Giger, M. L. (2019). Deep learning in medical imaging and radiation therapy. Medical Physics, 46(1), e1–e36.</p> <p><a href="https://doi.org/10.1002/mp.13264">Paper link on DOI</a></p>]]></content><author><name></name></author><category term="paper-summary,"/><category term="computer-vision,"/><category term="mia"/><summary type="html"><![CDATA[Background and Introduction]]></summary></entry><entry><title type="html">Paper Summary: Quality Assurance for AI-Based Applications in Radiation Therapy</title><link href="https://amithjkamath.github.io/blog/2024/qa-for-ai-in-radiotherapy/" rel="alternate" type="text/html" title="Paper Summary: Quality Assurance for AI-Based Applications in Radiation Therapy"/><published>2024-01-01T00:00:00+00:00</published><updated>2024-01-01T00:00:00+00:00</updated><id>https://amithjkamath.github.io/blog/2024/qa-for-ai-in-radiotherapy</id><content type="html" xml:base="https://amithjkamath.github.io/blog/2024/qa-for-ai-in-radiotherapy/"><![CDATA[<p>AI algorithms are typically data-driven, may be continuously evolving, and their behavior has a degree of (acceptable) uncertainty due to inherent noise in training data and the substantial number of parameters that are used in the algorithms.</p> <p>QA for AI-based systems is an emerging area, which has not been intensively explored and requires interactive collaborations between medical doctors, medical physics experts, and commercial/research AI institutions.</p> <p>Given their unique role as a bridge between the clinical environment and new technologies, medical physics experts are most likely the main frontiers to implementing these automated systems to improve efﬁciency, quality, standardization, and acceleration of the workﬂow leading to more safe and accurate radiation administration.</p> <p>It is, therefore, crucial to provide clear guidance for understanding and tackling the difﬁculties inherent in high-quality AI systems. This has been recognized by different societies in medical physics, which have recently published a detailed Checklist for AI in Medical Physics (CLAMP).</p> <p>Case-speciﬁc QA refers to all checks that verify the output of an AI-based application generated for each patient or machine.10 When QA results are satisfactory, the output can be used in the RT workﬂow. When the case-speciﬁc QA check fails, the output is subject to a second veriﬁcation. Depending on the application, the quality of the model’s output can be monitored in multiple ways. Currently, human supervision of the output, in combination with quantitative/qualitative measures, is seen as one of the most important tools. Automatic case-speciﬁc QA methods can be utilized to highlight divergent behavior. When too frequent failures are detected during case-specific QA, a routine QA is carried out to determine whether a recommissioning of the deployed model is needed.</p> <p>Routine QA is dedicated to the regular supervision of the AI model validity with the intention to monitor if the model’s output changes unexpectedly after clinical workﬂow changes (eg, software update, etc.). For this purpose, a periodical end-to-end performance should be completed on a reference test dataset. When routine QA tests do not meet expectations, a model re-commissioning may be necessary.</p> <h2 id="references">References</h2> <p>Claessens, M., Oria, C. S., Brouwer, C. L., Ziemer, B. P., Scholey, J. E., Lin, H., Witztum, A., Morin, O., Naqa, I. el, van Elmpt, W., &amp; Verellen, D. (2022). Quality Assurance for AI-Based Applications in Radiation Therapy. In Seminars in Radiation Oncology (Vol. 32, Issue 4, pp. 421–431). W.B. Saunders.</p> <p><a href="https://doi.org/10.1016/j.semradonc.2022.06.011">Paper link on DOI</a></p>]]></content><author><name></name></author><category term="paper-summary,"/><category term="computer-vision,"/><category term="mia"/><summary type="html"><![CDATA[AI algorithms are typically data-driven, may be continuously evolving, and their behavior has a degree of (acceptable) uncertainty due to inherent noise in training data and the substantial number of parameters that are used in the algorithms.]]></summary></entry><entry><title type="html">About Us: “I would like to convert my research into a useful tool for clinicians.” - Center for Artificial Intelligence in Medicine (CAIM)</title><link href="https://amithjkamath.github.io/blog/2022/about-us-i-would-like-to-convert-my-research-into-a-useful-tool-for-clinicians-center-for-artificial-intelligence-in-medicine-caim/" rel="alternate" type="text/html" title="About Us: “I would like to convert my research into a useful tool for clinicians.” - Center for Artificial Intelligence in Medicine (CAIM)"/><published>2022-12-01T00:00:00+00:00</published><updated>2022-12-01T00:00:00+00:00</updated><id>https://amithjkamath.github.io/blog/2022/about-us-i-would-like-to-convert-my-research-into-a-useful-tool-for-clinicians---center-for-artificial-intelligence-in-medicine-caim</id><content type="html" xml:base="https://amithjkamath.github.io/blog/2022/about-us-i-would-like-to-convert-my-research-into-a-useful-tool-for-clinicians-center-for-artificial-intelligence-in-medicine-caim/"><![CDATA[<p>December 2022Amith Kamath wishes to facilitate faster radiotherapy treatment for patients with glioblastoma through AI-supported therapy planning. The CAIM Young Researcher Award winner appreciates the openness of the Bernese community around AI applications in healthcare, also welcoming ideas from people trained in other disciplines to tackle hard problems in medicine. He is currently pursuing his PhD at the Medical Image Analysis research group of the ARTORG Center for Biomedical Engineering Research and looks forward to translating his research into a clinical tool through the broad entrepreneurial support he is receiving in Bern – including the personalized business coaching by be-advanced as part of his CAIM Award win in the category “translation”.What drives you in your research? My research is centered around evaluating the quality of radiotherapy delivered to patients with glioblastoma. Given the usually bad prognosis, people already diagnosed with this tumor currently must wait between one and three weeks until they can start treatment, due to the current workflows in radiotherapy planning. We expect that by using AI models to help draw boundaries around organs while simultaneously estimating the radiation dose and toxicity, radiotherapy treatment can be started earlier before the tumor has progressed further. We hope that someday our work can really add quality to people’s lives in this sense.The challenge in radiotherapy for glioblastoma is to be targeted while killing the tumor but sparing healthy areas of the brain. Mistakes made in these initial steps in the process can add up in subsequent steps, underscoring the importance of being precise. For example, if you irradiate healthy tissue in the brain, people can lose their functional abilities, for example speech or motor abilities. Our idea is to use deep neural networks to not only estimate, but also simulate inter-expert variations in boundaries that are drawn around tumors as well as healthy areas during radiotherapy planning. These simulations give us a better sense of the range of safe variations in how human experts manually do this and thereby understand the clinical impact of such variations in the process, leading to safer treatment.What does winning a CAIM Young Researcher Award mean to you? What matters to me most is that many of us were able to share our research and receive constructive feedback and comments in a setting like the CAIM Symposium. Beyond the award, the existence of such a vibrant community is very rewarding. For the translational focus of the award, I was fortunate to receive prior exposure through the Innosuisse startup toolbox program “Business Concepts” in October this year. The entrepreneurial coaching opportunity I now have with the CAIM Award is the perfect continuation of that. It would be great to get the experts’ opinion on how we can convert our research into a useful tool or product for clinicians!The unique thing here in Bern is the entrepreneurial support you receive, for example through the Swiss Institute for Translational and Entrepreneurial Medicine, sitem-insel. There is a lot of existing knowledge amongst the faculty about how a PhD project can be shaped into a product that can be used in clinics, which is quite exciting! If there are ten users of what I build, that will mean more to me than writing a long PhD thesis that no one may read.How important is it for you to share your research? Very much! My background is mostly in image processing and computer vision, and I think it’s great how welcoming the scientific community here is to researchers from other academic backgrounds. I don’t have a biomedicine background, and I believe people without medical schooling can make strong contributions to tackling hard problems in the medical space. Ways of thinking that are commonplace in another field could be novel to healthcare challenges and thus lead to innovative solutions. I like the people I get to work with daily who motivate me by asking all the right questions. I like that my work is very visual: I find it easier to look at a set of images or a video than at a bunch of equations for an “Aha” moment. When some images are hard to interpret, I appreciate that clinicians are quite open to talk to technical folks like us. This readiness to work with each other and speak the same language is quite important in this line of work. Therefore, I like to share our research with a broader global community. I use Social Media to exchange ideas with other scientists and our research lab Medical Image Analysis has started a “How to” video series for beginners in Deep Learning for medical imaging, summarizing some of the pitfalls and stumbling blocks in a humorous way (https://github.com/ubern-mia/bender). We are also currently preparing for a symposium on interpretability of AI models at CAIM in March ‘23, with the hope to get a lively discussion going on this important topic for safer AI adoption in medicine.Amith is a computer scientist and holds a Master of Science in Computer Science from Georgia Institute of Technology, in the US. He worked earlier as a software developer at the MathWorks Inc., on the Image Processing and Computer Vision Toolboxes in MATLAB, a scientific computing programming language. Prior to that, he earned a Master of Science in Electrical Engineering at University of Minnesota, and a Bachelor of Technology from National Institute of Technology Karnataka, in Surathkal, India. Currently, he is pursuing a PhD in Biomedical Engineering at the ARTORG Center at the University of Bern, under the supervision of Prof. Dr. Mauricio Reyes.His PhD thesis is on image segmentation and how one could use AI models to not only automate the otherwise time and effort intensive segmentation process, but further evaluate the quality of the contours in comparison to human-experts. His research focuses both on the robustness of using AI models to perform auto-segmentation, as well as computing radiotherapy dose predictions from these contours faster than current methods used in clinical practice. These results could help improve the speed as well as the quality and safety of radiotherapy plans for patients suffering from glioblastoma.Bern Interpretable AI Symposium (BIAS): www.caim.unibe.ch/bias2023</p>]]></content><author><name></name></author></entry><entry><title type="html">Paper Summary: Which Explanation Should I Choose? A Function Approximation Perspective to Characterizing Post hoc Explanations</title><link href="https://amithjkamath.github.io/blog/2022/Which-explanation-should-I-choose/" rel="alternate" type="text/html" title="Paper Summary: Which Explanation Should I Choose? A Function Approximation Perspective to Characterizing Post hoc Explanations"/><published>2022-11-16T00:00:00+00:00</published><updated>2022-11-16T00:00:00+00:00</updated><id>https://amithjkamath.github.io/blog/2022/Which-explanation-should-I-choose</id><content type="html" xml:base="https://amithjkamath.github.io/blog/2022/Which-explanation-should-I-choose/"><![CDATA[<p>This paper …</p> <p>Major contributions of this work include: -</p> <h1 id="major-learning-points">Major Learning Points</h1> <ol> <li></li> <li></li> </ol> <h1 id="interesting-bits">Interesting bits</h1> <ol> <li></li> <li></li> </ol> <h2 id="references">References</h2> <p><a href="https://arxiv.org/abs/2206.01254">Paper link on Arxiv</a></p> <p>Paper code doesn’t appear to be released yet, although is mentioned in the appendix.</p>]]></content><author><name></name></author><category term="paper-summary,"/><category term="computer-vision,"/><category term="mia"/><summary type="html"><![CDATA[This paper …]]></summary></entry><entry><title type="html">Paper Summary: Volumetric memory network for interactive medical image segmentation</title><link href="https://amithjkamath.github.io/blog/2022/Volumetric-memory-networks/" rel="alternate" type="text/html" title="Paper Summary: Volumetric memory network for interactive medical image segmentation"/><published>2022-11-03T00:00:00+00:00</published><updated>2022-11-03T00:00:00+00:00</updated><id>https://amithjkamath.github.io/blog/2022/Volumetric-memory-networks</id><content type="html" xml:base="https://amithjkamath.github.io/blog/2022/Volumetric-memory-networks/"><![CDATA[<p>This paper …</p> <p>Major contributions of this work include: -</p> <h1 id="major-learning-points">Major Learning Points</h1> <ol> <li></li> <li></li> </ol> <h1 id="interesting-bits">Interesting bits</h1> <ol> <li></li> <li></li> </ol> <h2 id="references">References</h2> <p><a href="https://www.sciencedirect.com/science/article/pii/S1361841522002316">Paper link on Sciencedirect</a></p> <p><a href="https://github.com/lingorX/Mem3D">Paper code</a></p>]]></content><author><name></name></author><category term="paper-summary,"/><category term="computer-vision,"/><category term="mia"/><summary type="html"><![CDATA[This paper …]]></summary></entry><entry><title type="html">Paper Summary: Calibrating Segmentation Networks with Margin Based Label Smoothing</title><link href="https://amithjkamath.github.io/blog/2022/Calibrating-segmentation-networks/" rel="alternate" type="text/html" title="Paper Summary: Calibrating Segmentation Networks with Margin Based Label Smoothing"/><published>2022-10-19T00:00:00+00:00</published><updated>2022-10-19T00:00:00+00:00</updated><id>https://amithjkamath.github.io/blog/2022/Calibrating-segmentation-networks</id><content type="html" xml:base="https://amithjkamath.github.io/blog/2022/Calibrating-segmentation-networks/"><![CDATA[<p>This paper tackles the problem of models that are poorly calibrated, which result in over-confident predictions. The problem with cross entropy based loss functions is that it promotes the predicted softmax probabilities to match the one-hot label assignments, which means that the correct label activation should be significantly larger than the remaining activations.</p> <p>Major contributions of this work include: - A unifying constrained-optimization perspective of current state-of-the-art calibration losses, which are approximations of a linear penalty (or a Lagrangian term) imposing equality constraints on logit distances. - A simple and flexible generalization based on inequality constraints, which imposes a controllable margin on logit distances. - Comprehensive experiments on a variety of public medical image segmentation benchmarks demonstrate novel state-of-the-art results for calibration, while also improving the discriminative performance.</p> <h1 id="major-learning-points">Major Learning Points</h1> <ol> <li> <p>There are many existing methods of improving calibration, including focal loss and label smoothing. The authors show in this paper that these could be viewed as different penalty functions for imposing the same logit-distance equality constraint “d(l) = 0” (where d is the logit distance to the winning class). The proposed margin-based generalization (d(l) ≤ m) of this logit-distance constraint is shown to have desirable properties like gradient dynamics for calibrating neural networks.</p> </li> <li> <p>The experiments are rather comprehensive, however some of the data sets are really small. For example the MRBrainS18 data set has 7 subjects and 5 are used as training, 2 as test. The ACDC data set is split into 70 training, 10 validation and 20 test, which could have better power to make reasonable inferences. With this context, the results cannot hence be compared apples to apples (in my opinion at least) with each other, as BRATS (which has 4x the subjects as ACDC) should be equally highly weighted while evaluating the metrics.</p> </li> </ol> <h1 id="interesting-bits">Interesting bits</h1> <ol> <li> <p>The calibration performance metrics this paper uses include ECE (Expected Calibration Error) and CECE (Classwise ECE). These attempt to address the problem that pseudo-probability of the predicted class almost always over-estimates the actual probability of getting a correct answer. For example, if the largest pseudo-probability is 0.95 you don’t have a 95% chance of making a correct prediction — more like 75% or 85% chance of a correct prediction. (see <a href="https://jamesmccaffrey.wordpress.com/2021/01/22/how-to-calculate-expected-calibration-error-for-multi-class-classification/">here</a> for a great explanation of how ECE is computed).</p> </li> <li> <p>The networks chosen to test out this novel loss formulation to improve calibration include the now classic Unet, in addition to attention-Unet, Unet++, and TransUNet. What is interesting here is that the Unet somehow forms the basis of all subsequent networks and in spite of being nearly a decade old now, is still by far one of the best performing models.</p> </li> </ol> <h2 id="references">References</h2> <p><a href="https://arxiv.org/abs/2209.09641">Paper link on Arxiv</a></p> <p><a href="https://github.com/Bala93/MarginLoss">Paper code</a></p>]]></content><author><name></name></author><category term="paper-summary,"/><category term="computer-vision,"/><category term="mia"/><summary type="html"><![CDATA[This paper tackles the problem of models that are poorly calibrated, which result in over-confident predictions. The problem with cross entropy based loss functions is that it promotes the predicted softmax probabilities to match the one-hot label assignments, which means that the correct label activation should be significantly larger than the remaining activations.]]></summary></entry><entry><title type="html">Paper Summary: Diffusion models beat GANs on Image Synthesis</title><link href="https://amithjkamath.github.io/blog/2022/Diffusion-models-beat-GANs/" rel="alternate" type="text/html" title="Paper Summary: Diffusion models beat GANs on Image Synthesis"/><published>2022-10-05T00:00:00+00:00</published><updated>2022-10-05T00:00:00+00:00</updated><id>https://amithjkamath.github.io/blog/2022/Diffusion-models-beat-GANs</id><content type="html" xml:base="https://amithjkamath.github.io/blog/2022/Diffusion-models-beat-GANs/"><![CDATA[<p>This paper shows that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. This is achieved in unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, sample quality is improved with classifier guidance: compute-efficient method for trading off diversity for fidelity using gradients from a classifier (which is one of the advantages of GANs).</p> <p>Major contributions of this work include: - Introducing adaptive group normalization (AdaGN), which incorporates the timestep and class embedding into each residual block after a group normalization operation - Using classifier guidance, so that as few as 25 forward passes can generate samples maintaining FIDs (Frechet Inception Distance) comparable to BigGAN. - Architecture improvements: more heads or fewer channels per head improves FID; 64 channels is best for wall-clock time, hence 64 channels per head is used as default.</p> <h1 id="major-learning-points">Major Learning Points</h1> <ol> <li> <p>Diffusion models are a class of likelihood-based models which have recently been shown to produce high-quality images while offering desirable properties such as distribution coverage, a stationary training objective, and easy scalability. These models generate samples by gradually removing noise from a signal, and their training objective can be expressed as a reweighted variational lower-bound. It is hypothesized that the gap between diffusion models and GANs stems from at least two factors: first, that the model architectures used by recent GANs have been heavily explored and refined; second, that GANs are able to trade off diversity for fidelity, producing high quality samples but not covering the whole distribution.</p> </li> <li> <p>The proposed diffusion model obtains the best FID on each task, and the best sFID on all but one task (among LSUN and ImageNet data sets). With the improved architecture, state-of-the-art image generation results are obtained on LSUN and ImageNet 64×64. For higher resolution ImageNet, classifier guidance allows these models to substantially outperform the best GANs. These models obtain perceptual quality similar to GANs, while maintaining a higher coverage of the distribution as measured by recall, and can even do so using only 25 diffusion steps.</p> </li> </ol> <h1 id="interesting-bits">Interesting bits</h1> <ol> <li> <p>(from the Wikipedia entry) The Fréchet inception distance (FID) is a metric used to assess the quality of images created by a generative model, like a generative adversarial network (GAN). Unlike the earlier inception score (IS), which evaluates only the distribution of generated images, the FID compares the distribution of generated images with the distribution of a set of real images (“ground truth”). The FID metric was introduced in 2017, and is the current standard metric for assessing the quality of generative models as of 2020. It has been used to measure the quality of many recent models including the high-resolution StyleGAN1 and StyleGAN2 networks. Rather than directly comparing images pixel by pixel (for example, as done by the L2 norm), the FID compares the mean and standard deviation of the deepest layer in Inception v3. These layers are closer to output nodes that correspond to real-world objects such as a specific breed of dog or an airplane, and further from the shallow layers near the input image.</p> </li> <li> <p>Diffusion models are an extremely promising direction for generative modeling, but they are still slower than GANs at sampling (inference) time due to the use of multiple denoising steps (and therefore forward passes). Luhman and Luhman explore a way to distill the DDIM (Denoising Diffusion Implicit Models) sampling process into a single step model. The samples from the single step model are not yet competitive with GANs, but are much better than previous single-step likelihood-based models. Future work in this direction might be able to completely close the sampling speed gap between diffusion models and GANs without sacrificing image quality.</p> </li> </ol> <h2 id="references">References</h2> <p><a href="https://arxiv.org/abs/2105.05233">Paper link on Arxiv</a></p> <p><a href="https://github.com/openai/guided-diffusion">Paper code</a></p>]]></content><author><name></name></author><category term="paper-summary,"/><category term="computer-vision,"/><category term="mia"/><summary type="html"><![CDATA[This paper shows that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. This is achieved in unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, sample quality is improved with classifier guidance: compute-efficient method for trading off diversity for fidelity using gradients from a classifier (which is one of the advantages of GANs).]]></summary></entry></feed>