<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://amithjkamath.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://amithjkamath.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-05-22T15:47:50+00:00</updated><id>https://amithjkamath.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">TaiRO: AI for Treatment Planning</title><link href="https://amithjkamath.github.io/blog/2025/07-TaiRO-Treatment-Planning/" rel="alternate" type="text/html" title="TaiRO: AI for Treatment Planning"/><published>2025-05-01T00:00:00+00:00</published><updated>2025-05-01T00:00:00+00:00</updated><id>https://amithjkamath.github.io/blog/2025/07-TaiRO-Treatment-Planning</id><content type="html" xml:base="https://amithjkamath.github.io/blog/2025/07-TaiRO-Treatment-Planning/"><![CDATA[<h1 id="7-ai-for-treatment-planning">7: AI for Treatment Planning</h1> <p>Treatment planning is a complex, iterative process that aims to deliver an optimal radiation dose to the target volume while minimizing dose to surrounding healthy tissues. This process traditionally requires significant expertise and time from medical physicists and dosimetrists. Deep learning approaches offer the potential to automate aspects of treatment planning, predict optimal dose distributions, and potentially improve plan quality and consistency while reducing planning time.</p> <h2 id="dose-prediction-models">Dose Prediction Models</h2> <p>Dose prediction models use deep learning to estimate the radiation dose distribution that would result from a treatment plan, without performing the full physics-based dose calculation. These models can serve as rapid approximations or as components in automated planning systems.</p> <h3 id="dvh-estimation">DVH Estimation</h3> <p>The Dose-Volume Histogram (DVH) is a fundamental tool in radiation therapy planning, summarizing the dose distribution within a structure as a curve showing the volume receiving at least a given dose. DVH-based metrics (e.g., V20 for lung, mean heart dose) are commonly used as plan evaluation criteria and predictors of toxicity risk.</p> <p>Deep learning models for DVH estimation aim to predict these curves or metrics directly from patient anatomy and treatment parameters, without requiring a complete treatment plan. These models typically:</p> <ol> <li> <p>Take as input the patient’s CT images, contours of target volumes and OARs, and potentially treatment parameters (e.g., prescription dose, treatment technique).</p> </li> <li> <p>Output predicted DVH curves or specific DVH metrics for targets and OARs.</p> </li> </ol> <p>Architectures for DVH prediction include:</p> <p>Convolutional neural networks (CNNs) that process 3D anatomical information to capture spatial relationships between structures.</p> <p>Hybrid models that combine CNNs for feature extraction with fully connected layers to predict DVH points or parameters.</p> <p>Graph neural networks that represent the spatial relationships between different anatomical structures and learn to predict their dosimetric interdependencies.</p> <p>DVH prediction models can serve multiple purposes:</p> <p>Pre-planning feasibility assessment: Quickly estimating whether dosimetric goals are achievable for a specific patient before detailed planning begins.</p> <p>Plan quality evaluation: Comparing achieved DVHs against predicted “optimal” DVHs to identify potential improvements.</p> <p>Treatment technique selection: Predicting DVHs for different modalities (e.g., IMRT vs. proton therapy) to guide the choice of technique.</p> <p>The accuracy of DVH prediction models depends on the consistency of planning practices in the training data. These models essentially learn to mimic the planning patterns and trade-offs made by the institution that generated the training plans.</p> <h3 id="dose-distribution-prediction">Dose Distribution Prediction</h3> <p>Beyond summary DVH metrics, deep learning models can predict the full 3D dose distribution within the patient. These models map from patient anatomy (and potentially treatment parameters) to voxel-wise dose values throughout the planning volume.</p> <p>Common architectures for dose prediction include:</p> <p>U-Net and its variants, which have proven effective for this task due to their ability to capture both local and global anatomical context. The encoder pathway processes the input images and contours, while the decoder generates the predicted dose distribution.</p> <p>Conditional Generative Adversarial Networks (cGANs), where the generator produces dose distributions conditioned on patient anatomy, and the discriminator learns to distinguish between real and generated dose distributions. This adversarial training can help produce more realistic dose distributions.</p> <p>Attention-based architectures that learn to focus on the most relevant anatomical features for predicting dose at each location.</p> <p>Dose prediction models face several challenges:</p> <p>Handling the high dynamic range of dose values, which can span several orders of magnitude.</p> <p>Accurately predicting dose in regions with complex tissue heterogeneity or at interfaces between different tissues.</p> <p>Capturing the impact of different beam arrangements, which may not be explicitly provided as input.</p> <p>Ensuring that predicted dose distributions satisfy physical constraints (e.g., dose cannot increase with depth beyond the build-up region for a single beam).</p> <p>Despite these challenges, dose prediction models have shown promising results, with many achieving mean absolute errors of less than 5% of the prescription dose in most regions. These models can serve as rapid approximations for plan evaluation, as starting points for optimization, or as components in fully automated planning systems.</p> <h2 id="plan-quality-assessment">Plan Quality Assessment</h2> <p>Evaluating the quality of radiation treatment plans is traditionally a manual process based on clinical experience and protocol-specific criteria. Deep learning approaches aim to automate and standardize this assessment, potentially improving plan consistency and quality.</p> <h3 id="automated-plan-evaluation">Automated Plan Evaluation</h3> <p>Automated plan evaluation models assess whether a treatment plan meets clinical goals and how it compares to historically “good” plans for similar cases. These models can:</p> <p>Classify plans as acceptable or requiring improvement based on dosimetric and geometric features.</p> <p>Score plans on a continuous scale, potentially highlighting specific aspects that could be improved.</p> <p>Compare a plan against a database of previous plans for similar patients to identify potential outliers or areas for improvement.</p> <p>Deep learning approaches to plan evaluation include:</p> <p>Supervised classification or regression models trained on expert-labeled plans.</p> <p>Anomaly detection models that identify unusual dose distributions or DVH characteristics.</p> <p>Reinforcement learning frameworks that learn to evaluate plans based on clinical outcomes or expert preferences.</p> <p>The features used for plan evaluation typically include:</p> <p>DVH metrics for targets and OARs Conformity and homogeneity indices Spatial dose characteristics (e.g., gradient measures, hot spots) Geometric relationships between dose distribution and anatomical structures</p> <p>Automated plan evaluation can serve as a quality assurance tool, providing an objective second check of plans before approval. It can also help identify systematic differences in planning approaches between institutions or planners, potentially leading to standardization of best practices.</p> <h3 id="knowledge-based-planning">Knowledge-based Planning</h3> <p>Knowledge-based planning (KBP) leverages historical treatment plans to predict achievable dose-volume objectives for new patients. While traditional KBP approaches often use statistical modeling, deep learning has enhanced these methods by capturing more complex relationships between patient anatomy and achievable dose distributions.</p> <p>Deep learning-based KBP models typically:</p> <ol> <li> <p>Extract features from the patient’s anatomy, particularly the spatial relationships between target volumes and OARs.</p> </li> <li> <p>Predict achievable dose-volume constraints based on these features and a database of previously delivered plans.</p> </li> <li> <p>Use these predictions to guide the optimization of new treatment plans.</p> </li> </ol> <p>Architectures for KBP include:</p> <p>CNNs that process anatomical features to predict achievable dose metrics.</p> <p>Recurrent neural networks (RNNs) that model the sequential nature of DVH curves.</p> <p>Attention mechanisms that focus on the most relevant historical plans or anatomical features for a new patient.</p> <p>KBP approaches have shown the ability to reduce planning time and improve plan quality, particularly for less experienced planners or centers. They can also help identify suboptimal plans by comparing achieved dose metrics against predicted achievable values.</p> <p>The effectiveness of KBP models depends on the quality and diversity of the historical plans used for training. These models essentially learn to reproduce the planning patterns in their training data, which may not always represent optimal planning.</p> <h2 id="automated-treatment-planning">Automated Treatment Planning</h2> <p>Fully automated treatment planning aims to generate complete, clinically acceptable plans with minimal human intervention. Deep learning approaches to automated planning range from enhancing specific steps in the planning workflow to end-to-end systems that generate deliverable plans directly from patient images and prescriptions.</p> <h3 id="knowledge-based-planning-1">Knowledge-based Planning</h3> <p>Building on the KBP concepts discussed earlier, deep learning can automate the entire planning process by:</p> <ol> <li> <p>Predicting optimal dose distributions based on patient anatomy and prescription.</p> </li> <li> <p>Generating beam parameters (angles, shapes, weights) that would produce the desired dose distribution.</p> </li> <li> <p>Optimizing the final plan to meet clinical goals while ensuring deliverability.</p> </li> </ol> <p>Several approaches have been developed:</p> <p>Two-stage models first predict an “ideal” dose distribution, then use inverse planning to determine the machine parameters needed to achieve it.</p> <p>End-to-end models directly predict deliverable plan parameters from patient anatomy.</p> <p>Hybrid approaches combine machine learning for certain steps (e.g., beam angle selection) with conventional optimization for others (e.g., fluence map optimization).</p> <p>The advantages of automated planning include:</p> <p>Reduced planning time, potentially enabling more adaptive approaches. Consistent plan quality, reducing variability between planners. Ability to rapidly explore multiple planning strategies. Potential for improved plan quality by learning from the best examples in the training data.</p> <p>Challenges include ensuring that the generated plans are physically deliverable, clinically acceptable across diverse patient anatomies, and robust to uncertainties in patient setup and internal motion.</p> <h3 id="reinforcement-learning-approaches">Reinforcement Learning Approaches</h3> <p>Reinforcement learning (RL) offers a promising framework for automated treatment planning, as it can directly optimize for clinical objectives without requiring labeled “optimal” plans for training. In an RL framework:</p> <p>The state represents the current dose distribution, patient anatomy, and planning constraints. Actions include adjusting beam parameters, optimization weights, or other planning variables. Rewards are based on dosimetric criteria, conforming to clinical goals for target coverage and OAR sparing.</p> <p>RL approaches to treatment planning include:</p> <p>Beam angle selection, where the agent learns to sequentially select optimal beam directions. Fluence map optimization, where the agent directly adjusts intensity patterns to optimize the dose distribution. Multi-criteria optimization, where the agent learns to navigate trade-offs between competing objectives.</p> <p>The advantage of RL is its ability to discover novel planning strategies that might not be present in historical data. However, challenges include:</p> <p>Defining appropriate reward functions that accurately reflect clinical priorities. Ensuring safe exploration during training, typically by using simulation environments. Managing the large state and action spaces involved in treatment planning.</p> <p>Research in RL for treatment planning is still emerging, with most applications focusing on specific aspects of the planning process rather than end-to-end solutions. As these methods mature, they could potentially discover novel planning approaches that outperform current clinical practice.</p> <h2 id="multi-criteria-optimization">Multi-criteria Optimization</h2> <p>Radiation therapy planning inherently involves balancing multiple competing objectives: maximizing tumor control while minimizing toxicity to various normal tissues. Multi-criteria optimization (MCO) explicitly addresses these trade-offs, and deep learning approaches are enhancing this process.</p> <h3 id="traditional-mco-approaches">Traditional MCO Approaches</h3> <p>Traditional MCO in radiation therapy generates a set of Pareto-optimal plans, where no objective can be improved without worsening another. Planners then navigate this Pareto surface to select a plan that best balances the clinical trade-offs.</p> <p>Deep learning is enhancing MCO in several ways:</p> <p>Predicting the Pareto surface more efficiently than generating multiple plans through conventional optimization. Learning to navigate the Pareto surface based on historical planner preferences or clinical outcomes. Identifying the most relevant trade-offs to explore for a specific patient anatomy.</p> <p>Architectures for MCO include:</p> <p>Generative models that can rapidly produce dose distributions at different points on the Pareto surface. Recommendation systems that suggest promising regions of the Pareto surface to explore. Interactive systems that learn from planner feedback to refine the presented trade-offs.</p> <p>These approaches aim to make MCO more efficient and intuitive, potentially enabling more widespread clinical adoption of this powerful planning approach.</p> <h3 id="balancing-clinical-priorities">Balancing Clinical Priorities</h3> <p>Beyond technical optimization, deep learning can help balance clinical priorities by:</p> <p>Learning from historical decisions how different institutions or physicians weigh various clinical factors. Predicting patient-specific risks of different toxicities to inform trade-off decisions. Incorporating non-dosimetric factors (e.g., patient comorbidities, concurrent treatments) into planning decisions.</p> <p>These approaches recognize that optimal planning involves more than just meeting generic dosimetric constraints—it requires considering the specific clinical context and patient characteristics.</p> <h2 id="current-research-in-treatment-planning-prediction">Current Research in Treatment Planning Prediction</h2> <p>Research in deep learning for treatment planning continues to advance rapidly, addressing remaining challenges and expanding capabilities.</p> <h3 id="plan-parameter-optimization">Plan Parameter Optimization</h3> <p>Current research in plan parameter optimization focuses on:</p> <p>Beam angle optimization: Developing models that can predict optimal beam arrangements based on patient anatomy, potentially considering non-coplanar beams and novel delivery techniques.</p> <p>Fluence map prediction: Directly predicting optimal fluence patterns that balance target coverage and OAR sparing, potentially reducing the need for iterative optimization.</p> <p>Segment shape optimization: For direct machine parameter optimization, predicting deliverable multi-leaf collimator (MLC) shapes and weights.</p> <p>Hyperparameter tuning: Automatically selecting optimization parameters (e.g., objective weights, priorities) that lead to high-quality plans.</p> <p>These approaches aim to streamline the planning process by reducing the need for manual parameter selection and iterative refinement.</p> <h3 id="beam-angle-selection">Beam Angle Selection</h3> <p>Beam angle selection remains a challenging aspect of treatment planning, particularly for complex techniques like non-coplanar IMRT or VMAT. Deep learning approaches include:</p> <p>Classification models that predict whether a potential beam angle would be beneficial for a specific patient.</p> <p>Reinforcement learning agents that sequentially select beam angles to optimize overall plan quality.</p> <p>Graph neural networks that model the geometric relationships between target volumes, OARs, and potential beam paths.</p> <p>Attention-based models that learn to focus on the most relevant anatomical features for beam selection.</p> <p>These models aim to automate a process that traditionally relies heavily on planner experience and can significantly impact plan quality.</p> <h3 id="adaptive-planning">Adaptive Planning</h3> <p>Adaptive radiation therapy, where plans are modified during the treatment course to account for anatomical changes, presents unique challenges and opportunities for deep learning:</p> <p>Predicting anatomical changes: Models that forecast how a patient’s anatomy might change during treatment based on early observations, enabling proactive plan adaptation.</p> <p>Automated replanning: Rapidly generating adapted plans based on new imaging, potentially enabling online adaptation.</p> <p>Dose accumulation: Accurately estimating the total delivered dose across multiple fractions with changing anatomy.</p> <p>Decision support: Predicting which patients would benefit most from plan adaptation based on observed anatomical changes.</p> <p>These applications could help make adaptive radiotherapy more practical and widely available by reducing the time and resources required for replanning.</p> <h3 id="integration-with-outcome-prediction">Integration with Outcome Prediction</h3> <p>An emerging research direction is the integration of treatment planning with outcome prediction:</p> <p>Outcome-aware planning: Directly optimizing plans to maximize predicted tumor control and minimize predicted toxicity, rather than using generic dosimetric constraints.</p> <p>Personalized planning: Tailoring plans to individual patient characteristics that might affect radiosensitivity or toxicity risk.</p> <p>Radiomics-guided planning: Using radiomic features extracted from pre-treatment images to identify tumor subregions that might benefit from dose escalation or de-escalation.</p> <p>These approaches aim to move beyond one-size-fits-all planning to truly personalized radiation therapy that considers the unique characteristics of each patient and tumor.</p> <p>As deep learning models for treatment planning continue to improve in accuracy, robustness, and clinical relevance, they hold the potential to significantly enhance the efficiency, consistency, and quality of radiation therapy planning. The integration of these models into clinical workflows, combined with appropriate quality assurance and human oversight, could ultimately improve outcomes for cancer patients receiving radiation therapy.</p>]]></content><author><name></name></author><category term="tairo,"/><category term="radiation-oncology,"/><category term="artificial-intelligence"/><summary type="html"><![CDATA[7: AI for Treatment Planning]]></summary></entry><entry><title type="html">TaiRO: AI for Contouring</title><link href="https://amithjkamath.github.io/blog/2025/04-TaiRO-Contouring/" rel="alternate" type="text/html" title="TaiRO: AI for Contouring"/><published>2025-04-01T00:00:00+00:00</published><updated>2025-04-01T00:00:00+00:00</updated><id>https://amithjkamath.github.io/blog/2025/04-TaiRO-Contouring</id><content type="html" xml:base="https://amithjkamath.github.io/blog/2025/04-TaiRO-Contouring/"><![CDATA[<h1 id="4-ai-for-image-contouring">4: AI for Image Contouring</h1> <p>Accurate delineation of target volumes and organs at risk (OARs) is a critical and often time-consuming step in radiation therapy planning. Manual contouring is subject to inter-observer variability and can be a significant bottleneck in the clinical workflow. Deep learning, particularly convolutional neural networks (CNNs), has shown remarkable success in automating this process, offering the potential for increased efficiency, consistency, and accuracy.</p> <h2 id="contouring-fundamentals">Contouring Fundamentals</h2> <p>Contouring in radiation oncology is fundamentally a segmentation task—assigning a label (e.g., tumor, specific OAR, background) to each voxel in a 3D medical image (typically CT or MRI). Understanding the different types of segmentation is essential for applying deep learning effectively.</p> <h3 id="binary-vs-multi-class-segmentation">Binary vs. Multi-class Segmentation</h3> <p>Binary segmentation involves distinguishing between two classes: the object of interest (foreground) and everything else (background). For example, segmenting a single OAR like the heart involves classifying each voxel as either “heart” or “not heart.”</p> <p>Multi-class segmentation extends this to multiple distinct classes simultaneously. In radiation oncology, this is common when contouring multiple OARs and potentially the target volume within the same image. The model must assign each voxel to one of several predefined classes (e.g., heart, lung, spinal cord, tumor, background).</p> <p>Deep learning models for segmentation typically output a probability map for each class. For binary segmentation, a single output channel with sigmoid activation provides the probability of belonging to the foreground class. For multi-class segmentation, multiple output channels with softmax activation provide probabilities for each class, ensuring they sum to one for each voxel.</p> <h3 id="semantic-vs-instance-segmentation">Semantic vs. Instance Segmentation</h3> <p>Semantic segmentation assigns a class label to each voxel but does not distinguish between different instances of the same class. For example, if multiple lymph nodes are present, semantic segmentation would label all of them as “lymph node” without differentiating individual nodes.</p> <p>Instance segmentation goes a step further by identifying and delineating each individual object instance. In the lymph node example, instance segmentation would provide a separate contour for each distinct node.</p> <p>While most contouring tasks in radiation oncology currently rely on semantic segmentation (delineating specific organs or tumor volumes), instance segmentation could be relevant for tasks like identifying individual metastatic lesions or tracking multiple objects over time.</p> <h2 id="segmentation-architectures">Segmentation Architectures</h2> <p>Several deep learning architectures have proven particularly effective for medical image segmentation, building upon the foundational concepts of CNNs and attention mechanisms discussed in Module 4.</p> <h3 id="u-net-and-variants">U-Net and Variants</h3> <p>The U-Net architecture, specifically designed for biomedical image segmentation, remains the cornerstone of many contouring applications. Its encoder-decoder structure with skip connections effectively combines multi-scale feature extraction with precise spatial localization.</p> <p>Key features of U-Net include:</p> <p>Symmetric encoder-decoder paths: The encoder progressively reduces spatial resolution and increases feature channels, while the decoder symmetrically increases resolution and decreases channels.</p> <p>Skip connections: Concatenating feature maps from the encoder to corresponding layers in the decoder allows the network to reuse high-resolution features, crucial for accurate boundary delineation.</p> <p>Overlap-tile strategy (optional): For large images, U-Net can process overlapping patches and seamlessly combine the predictions.</p> <p>Numerous variants have built upon the U-Net foundation:</p> <p>Residual U-Net incorporates residual connections within the convolutional blocks, potentially improving gradient flow and enabling deeper networks.</p> <p>Attention U-Net integrates attention gates into the skip connections, allowing the model to selectively focus on relevant features being passed from the encoder to the decoder.</p> <p>U-Net++ uses nested and dense skip connections to further bridge the semantic gap between encoder and decoder features, potentially improving performance on complex segmentation tasks.</p> <p>These U-Net based architectures form the backbone of many automated contouring systems used or investigated in radiation oncology clinics.</p> <h3 id="v-net-for-3d-segmentation">V-Net for 3D Segmentation</h3> <p>While 2D U-Nets process images slice by slice, medical imaging data in radiation oncology is inherently 3D (CT, MRI volumes). V-Net extends the U-Net concept to fully 3D convolutions, allowing the network to directly leverage spatial context across slices.</p> <p>V-Net replaces 2D convolutions, pooling, and up-sampling operations with their 3D counterparts. It often incorporates residual connections within its convolutional blocks. By processing the entire 3D volume (or large 3D patches), V-Net can better capture the complex shapes and relationships of anatomical structures in three dimensions, potentially leading to more accurate and consistent segmentations compared to slice-by-slice 2D approaches.</p> <p>The main challenge with 3D architectures like V-Net is the significantly increased computational cost and memory requirements due to the cubic growth in data size. This often necessitates processing smaller 3D patches or using techniques like downsampling to manage resource constraints.</p> <h3 id="attention-based-segmentation">Attention-based Segmentation</h3> <p>Attention mechanisms, particularly self-attention as used in transformers, can enhance segmentation models by allowing them to capture long-range dependencies and focus on relevant image regions.</p> <p>Several architectures incorporate attention:</p> <p>Attention U-Net (mentioned earlier) uses attention gates in skip connections.</p> <p>Transformer-based segmentation models like UNETR and Swin UNETR replace or augment parts of the U-Net architecture (typically the encoder) with transformer blocks. These models can capture global context more effectively than pure CNN approaches, which can be beneficial for segmenting large or complex structures.</p> <p>Non-local networks incorporate self-attention modules within convolutional architectures to capture long-range spatial dependencies.</p> <p>Attention mechanisms can help models better understand the global context of an image, differentiate between similar-looking tissues based on surrounding structures, and improve segmentation accuracy, particularly for challenging cases with ambiguous boundaries or anatomical variations.</p> <h2 id="loss-functions-for-segmentation">Loss Functions for Segmentation</h2> <p>The choice of loss function significantly impacts how a segmentation model learns and the quality of the resulting contours. Standard classification losses like cross-entropy can be suboptimal for segmentation, especially with imbalanced data.</p> <h3 id="dice-loss">Dice Loss</h3> <p>Dice loss, derived from the Dice Similarity Coefficient (DSC), directly optimizes for overlap between the predicted and ground truth segmentations. It is defined as:</p> <p>L_Dice = 1 - DSC = 1 - (2 * ∑(p_i * g_i) + ε) / (∑p_i + ∑g_i + ε)</p> <p>Where p_i is the predicted probability for voxel i, g_i is the ground truth label (0 or 1), and ε is a small constant for numerical stability.</p> <p>Dice loss is particularly effective for imbalanced segmentation tasks because it focuses on the agreement between foreground predictions and ground truth, regardless of the number of background voxels. It has become a standard loss function for medical image segmentation.</p> <p>Variations like Generalized Dice Loss handle multi-class segmentation by weighting the contribution of each class based on its volume, preventing larger structures from dominating the loss.</p> <h3 id="focal-loss">Focal Loss</h3> <p>Focal loss, originally proposed for object detection, modifies the standard cross-entropy loss to down-weight the contribution of easy-to-classify examples (often the abundant background voxels) and focus training on harder examples (often foreground voxels or boundary regions):</p> <p>L_Focal = -α(1-p_t)^γ * log(p_t)</p> <p>Where p_t is the probability of the correct class, α balances class importance, and γ is the focusing parameter. Higher values of γ increase the focus on hard examples.</p> <p>Focal loss can be effective in segmentation tasks with extreme class imbalance, helping the model learn to correctly classify rare foreground structures.</p> <h3 id="boundary-aware-losses">Boundary-aware Losses</h3> <p>Accurate boundary delineation is often critical in radiation oncology. Boundary-aware losses explicitly penalize errors near the segmentation boundaries:</p> <p>Boundary Loss computes the discrepancy between the predicted boundary and the ground truth boundary, often using distance transforms.</p> <p>Weighted Cross-Entropy or Dice Loss can assign higher weights to voxels near the boundary, forcing the model to pay more attention to these critical regions.</p> <p>Shape-aware losses incorporate prior knowledge about the expected shape of the structure being segmented, penalizing predictions that deviate significantly from plausible shapes.</p> <h3 id="combined-losses">Combined Losses</h3> <p>In practice, combining multiple loss functions often yields the best results. Common combinations include:</p> <p>Dice + Cross-Entropy: Balances overlap-based optimization with pixel-wise classification accuracy.</p> <p>Dice + Focal Loss: Combines overlap optimization with focused learning on hard examples.</p> <p>Adding boundary loss terms to Dice or cross-entropy losses to specifically improve boundary accuracy.</p> <p>The optimal loss function or combination depends on the specific segmentation task, dataset characteristics, and clinical priorities for contouring accuracy.</p> <h2 id="evaluation-metrics-for-contouring">Evaluation Metrics for Contouring</h2> <p>Evaluating the performance of automated contouring models requires metrics that capture clinically relevant aspects of segmentation quality, going beyond simple overlap measures.</p> <h3 id="geometric-metrics">Geometric Metrics</h3> <p>Dice Similarity Coefficient (DSC) and Intersection over Union (IoU) remain standard metrics for assessing overall overlap.</p> <p>Hausdorff Distance (HD) measures the maximum distance between the surfaces of the predicted and ground truth contours, quantifying the largest boundary discrepancy. The 95th percentile HD (HD95) is often preferred as it is less sensitive to outliers.</p> <p>Average Symmetric Surface Distance (ASSD) calculates the average distance between the surfaces, providing a measure of overall boundary agreement.</p> <p>Volume Difference measures the percentage difference between the predicted and ground truth volumes, indicating whether the model tends to under- or overestimate the structure size.</p> <h3 id="clinical-acceptability-measures">Clinical Acceptability Measures</h3> <p>While geometric metrics provide quantitative measures, clinical acceptability often involves more nuanced assessment:</p> <p>Contour smoothness and regularity: Automated contours should ideally be smooth and anatomically plausible, without jagged edges or unrealistic shapes.</p> <p>Boundary adherence: The contour should accurately follow the visible boundary of the structure in the image.</p> <p>Inclusion of critical regions / Exclusion of nearby structures: The contour must reliably include the entire target or OAR while avoiding encroachment on adjacent critical structures.</p> <p>Rating scales or qualitative assessments by expert clinicians are often used alongside geometric metrics to evaluate the clinical usability of automated contours. Studies may measure the percentage of automatically generated contours that require no edits, minor edits, or major edits by a clinician.</p> <h3 id="inter-observer-variability">Inter-observer Variability</h3> <p>It is crucial to compare the performance of automated contouring models against the inherent variability observed between human experts. A model that achieves performance comparable to inter-observer agreement is often considered clinically acceptable.</p> <p>Metrics like DSC, HD, and ASSD can be calculated between contours drawn by different clinicians on the same images to establish a baseline for human variability. The automated model’s performance is then compared to this baseline.</p> <p>Evaluating against multiple expert delineations (e.g., using the STAPLE algorithm to estimate a consensus ground truth) provides a more robust assessment than comparing against a single expert contour.</p> <h2 id="current-research-in-auto-contouring">Current Research in Auto-contouring</h2> <p>Research in deep learning for auto-contouring continues to advance rapidly, addressing remaining challenges and expanding capabilities.</p> <h3 id="organ-at-risk-oar-contouring">Organ-at-Risk (OAR) Contouring</h3> <p>Automated contouring of OARs is one of the most mature applications of deep learning in radiation oncology. Models have demonstrated high accuracy for many common OARs across different anatomical sites (head and neck, thorax, abdomen, pelvis).</p> <p>Current research focuses on:</p> <p>Improving robustness across different imaging protocols, scanners, and patient populations.</p> <p>Handling challenging OARs with low contrast or ambiguous boundaries.</p> <p>Developing models that can segment a comprehensive set of OARs simultaneously.</p> <p>Integrating uncertainty estimation to flag contours that may require manual review.</p> <p>Validating performance in large-scale, multi-institutional studies.</p> <h3 id="tumor-volume-delineation">Tumor Volume Delineation</h3> <p>Segmenting the gross tumor volume (GTV) and clinical target volume (CTV) is often more challenging than OAR contouring due to the variability in tumor appearance, infiltration patterns, and reliance on multi-modal imaging (e.g., PET-CT, MRI).</p> <p>Research efforts include:</p> <p>Developing multi-modal segmentation models that effectively fuse information from different imaging sources.</p> <p>Incorporating prior knowledge about tumor growth patterns or anatomical constraints.</p> <p>Addressing the significant inter-observer variability in tumor delineation.</p> <p>Predicting microscopic tumor extension for CTV definition.</p> <p>Using deep learning to identify tumor subregions with different biological characteristics (e.g., hypoxia, proliferation) based on imaging features (radiomics).</p> <h3 id="adaptive-contouring">Adaptive Contouring</h3> <p>During a course of radiation therapy, patient anatomy can change due to tumor shrinkage, weight loss, or organ motion. Adaptive radiotherapy requires re-contouring on images acquired during treatment (e.g., cone-beam CT).</p> <p>Deep learning models are being developed for:</p> <p>Deformable image registration to propagate initial contours to subsequent images.</p> <p>Direct segmentation on daily or weekly images, potentially adapting to changes over time.</p> <p>Predicting future anatomical changes to enable proactive plan adaptation.</p> <p>Challenges include the lower image quality of cone-beam CT compared to planning CT and the need for rapid processing to enable online adaptation.</p> <h3 id="quality-assurance-and-clinical-integration">Quality Assurance and Clinical Integration</h3> <p>A critical area of research involves developing methods for quality assurance (QA) of automated contours and facilitating safe clinical integration:</p> <p>Developing AI-based QA tools that can automatically flag potentially erroneous contours for human review.</p> <p>Quantifying the impact of auto-contouring on downstream treatment planning and predicted outcomes.</p> <p>Designing optimal workflows that combine automated contouring with efficient human review and editing.</p> <p>Establishing best practices for commissioning, validating, and monitoring auto-contouring systems in clinical practice.</p> <p>As deep learning models for auto-contouring continue to improve in accuracy and robustness, their integration into clinical workflows holds the potential to significantly enhance the efficiency and consistency of radiation therapy planning.</p>]]></content><author><name></name></author><category term="tairo,"/><category term="radiation-oncology,"/><category term="artificial-intelligence"/><summary type="html"><![CDATA[4: AI for Image Contouring]]></summary></entry><entry><title type="html">TaiRO: Medical Imaging in Radiation Oncology</title><link href="https://amithjkamath.github.io/blog/2025/03-TaiRO-Imaging/" rel="alternate" type="text/html" title="TaiRO: Medical Imaging in Radiation Oncology"/><published>2025-03-01T00:00:00+00:00</published><updated>2025-03-01T00:00:00+00:00</updated><id>https://amithjkamath.github.io/blog/2025/03-TaiRO-Imaging</id><content type="html" xml:base="https://amithjkamath.github.io/blog/2025/03-TaiRO-Imaging/"><![CDATA[<table> <thead> <tr> <th>Modality</th> <th>Typical Uses</th> <th>Duration</th> <th>Pros</th> <th>Cons</th> </tr> </thead> <tbody> <tr> <td>X-ray</td> <td>Fracture diagnosis; Lung infection detection; Dental evaluation</td> <td>10–15 minutes</td> <td>Quick and accessible; Relatively low cost; Effective for detecting fractures and lung conditions</td> <td>Limited soft tissue detail; Exposure to ionizing radiation</td> </tr> <tr> <td>Fluoroscopy</td> <td>Barium enema procedures; Cardiac catheterization; Joint injections</td> <td>30 minutes – 2 hours</td> <td>Real-time imaging; Guidance during procedures</td> <td>Exposure to ionizing radiation; Potential for contrast dye reactions</td> </tr> <tr> <td>CT Scan</td> <td>Tumor detection and staging; Vascular disease evaluation; Internal injury assessment</td> <td>20–25 minutes</td> <td>High-resolution images; Fast scanning times; Excellent for bone and vascular evaluation</td> <td>Higher radiation dose than X-rays; Contrast dye may be required</td> </tr> <tr> <td>MRI</td> <td>Brain and spinal cord imaging; Soft tissue evaluation; Multiple sclerosis diagnosis</td> <td>45 minutes – 1 hour</td> <td>Detailed soft tissue images; No ionizing radiation; Multiplanar imaging capabilities</td> <td>Longer scanning times; Claustrophobic for some patients; Limited availability for certain conditions</td> </tr> <tr> <td>Ultrasound</td> <td>Prenatal imaging and monitoring; Abdominal and pelvic evaluation; Cardiac and vascular imaging</td> <td>30 minutes – 1 hour</td> <td>Real-time imaging; No ionizing radiation; Safe for pregnant women</td> <td>Operator-dependent; Limited penetration for deep structures</td> </tr> <tr> <td>PET Scan</td> <td>Cancer diagnosis and staging; Brain function evaluation; Heart disease assessment</td> <td>1.5 – 2 hours</td> <td>Functional and metabolic information; Detection of small lesions; Accurate staging of cancers</td> <td>High cost; Limited availability; Requires radiotracer administration</td> </tr> <tr> <td>Mammography</td> <td>Breast cancer screening; Detection of breast abnormalities</td> <td>30 minutes</td> <td>Early detection of breast cancer; High-resolution images</td> <td>Slight discomfort during the procedure</td> </tr> </tbody> </table> <ol> <li> <p>X-Ray How it works: Uses ionizing radiation to produce 2D images of the body. Use cases: Bone fractures, lung infections (like pneumonia), chest imaging, dental exams. Speed: Very fast (few minutes). Cost: Low. Radiation: Low dose. Limitations: Poor soft tissue contrast; overlapping structures can obscure details.</p> </li> <li> <p>CT (Computed Tomography) How it works: Combines X-ray images taken from different angles into cross-sectional views. Use cases: Trauma, cancer, stroke, internal bleeding. Speed: Fast (minutes). Cost: Moderate to high. Radiation: Higher than X-ray (can be 100–1,000 times more). Strengths: Great for bone, chest, and detecting bleeding. Limitations: High radiation; contrast dye may affect kidneys.</p> </li> <li> <p>MRI (Magnetic Resonance Imaging) How it works: Uses magnets and radio waves to produce detailed images of soft tissues. Use cases: Brain, spinal cord, muscles, joints, tumors. Speed: Slower (30–90 minutes). Cost: High. Radiation: None. Strengths: Best soft tissue contrast; no radiation. Limitations: Claustrophobia, loud, not suitable with metal implants.</p> </li> <li> <p>Ultrasound How it works: High-frequency sound waves create real-time images. Use cases: Pregnancy, abdomen, heart (echocardiogram), blood flow. Speed: Real-time (immediate). Cost: Low to moderate. Radiation: None. Strengths: Safe in pregnancy, portable, real-time imaging. Limitations: Operator-dependent; limited by gas or obesity.</p> </li> <li> <p>Nuclear Medicine (e.g., PET, SPECT) How it works: Injects small amounts of radioactive material to assess function (not just structure). Use cases: Cancer detection, heart disease, brain disorders. Speed: Variable (can take hours). Cost: High. Radiation: Moderate to high (depends on tracer). Strengths: Functional imaging; early disease detection. Limitations: Radiation exposure; long prep and scan times.</p> </li> </ol> <p>“How tomographic reconstruction works?”: <a href="https://www.youtube.com/watch?v=f0sxjhGHRPo">This video</a> inspired by 3Blue1Brown shows how CT images are reconstructed in 3D using a series of single plane projections.</p> <p>“Radiology Modalities Explained: Understanding Medical Imaging Techniques”: <a href="https://ccdcare.com/resource-center/radiology-modalities/">This article</a> includes an overview of various radiology modalities, including X-rays, CT scans, MRI, ultrasound, and nuclear medicine. It explains how each modality works, their diagnostic applications, and considerations regarding radiation exposure.</p>]]></content><author><name></name></author><category term="tairo,"/><category term="radiation-oncology,"/><category term="artificial-intelligence"/><summary type="html"><![CDATA[Modality Typical Uses Duration Pros Cons X-ray Fracture diagnosis; Lung infection detection; Dental evaluation 10–15 minutes Quick and accessible; Relatively low cost; Effective for detecting fractures and lung conditions Limited soft tissue detail; Exposure to ionizing radiation Fluoroscopy Barium enema procedures; Cardiac catheterization; Joint injections 30 minutes – 2 hours Real-time imaging; Guidance during procedures Exposure to ionizing radiation; Potential for contrast dye reactions CT Scan Tumor detection and staging; Vascular disease evaluation; Internal injury assessment 20–25 minutes High-resolution images; Fast scanning times; Excellent for bone and vascular evaluation Higher radiation dose than X-rays; Contrast dye may be required MRI Brain and spinal cord imaging; Soft tissue evaluation; Multiple sclerosis diagnosis 45 minutes – 1 hour Detailed soft tissue images; No ionizing radiation; Multiplanar imaging capabilities Longer scanning times; Claustrophobic for some patients; Limited availability for certain conditions Ultrasound Prenatal imaging and monitoring; Abdominal and pelvic evaluation; Cardiac and vascular imaging 30 minutes – 1 hour Real-time imaging; No ionizing radiation; Safe for pregnant women Operator-dependent; Limited penetration for deep structures PET Scan Cancer diagnosis and staging; Brain function evaluation; Heart disease assessment 1.5 – 2 hours Functional and metabolic information; Detection of small lesions; Accurate staging of cancers High cost; Limited availability; Requires radiotracer administration Mammography Breast cancer screening; Detection of breast abnormalities 30 minutes Early detection of breast cancer; High-resolution images Slight discomfort during the procedure]]></summary></entry><entry><title type="html">TaiRO: Fundamentals of Deep Learning</title><link href="https://amithjkamath.github.io/blog/2025/02-TaiRO-Fundamentals/" rel="alternate" type="text/html" title="TaiRO: Fundamentals of Deep Learning"/><published>2025-02-01T00:00:00+00:00</published><updated>2025-02-01T00:00:00+00:00</updated><id>https://amithjkamath.github.io/blog/2025/02-TaiRO-Fundamentals</id><content type="html" xml:base="https://amithjkamath.github.io/blog/2025/02-TaiRO-Fundamentals/"><![CDATA[<h1 id="2-fundamentals-of-artificial-intelligence-through-deep-learning">2: Fundamentals of Artificial Intelligence through Deep Learning</h1> <h2 id="the-perceptron-model">The Perceptron Model</h2> <p>The perceptron represents the fundamental building block of neural networks and serves as an excellent starting point for understanding how these complex systems function. Developed in the late 1950s by Frank Rosenblatt, the perceptron was one of the earliest models of artificial neurons, inspired by the biological neurons in the human brain.</p> <p>At its core, a perceptron takes multiple input signals, applies weights to these inputs, sums them together with a bias term, and then passes this sum through an activation function to produce an output. Mathematically, this can be represented as:</p> <p>y = f(∑(w_i * x_i) + b)</p> <p>Where x_i represents the input features, w_i represents the corresponding weights, b is the bias term, and f is the activation function. In the original perceptron model, the activation function was a simple step function that output 1 if the weighted sum exceeded a threshold and 0 otherwise.</p> <p>The perceptron’s significance lies in its ability to learn from data through a simple update rule. When the perceptron makes an incorrect prediction, the weights are adjusted proportionally to the error and the input values. This learning process continues until the perceptron correctly classifies all training examples or reaches a maximum number of iterations.</p> <p>Despite its simplicity, the perceptron can solve linearly separable problems—those where a single straight line (or hyperplane in higher dimensions) can separate the different classes. This capability makes it suitable for basic classification tasks, such as distinguishing between different tissue types based on a few radiological features.</p> <p>However, the perceptron has significant limitations. As Marvin Minsky and Seymour Papert demonstrated in their 1969 book “Perceptrons,” a single perceptron cannot solve problems that are not linearly separable, such as the XOR problem. This limitation arises because a single perceptron can only represent a linear decision boundary.</p> <p>In radiation oncology, many problems involve complex, non-linear relationships between features and outcomes. For instance, the relationship between radiation dose and tumor control probability follows a sigmoid curve rather than a straight line. Similarly, the interaction between dose distribution and normal tissue complication probability involves complex, non-linear relationships that a single perceptron cannot capture.</p> <p>These limitations led to the development of multi-layer perceptrons (MLPs) or feedforward neural networks, which overcome the linear separability constraint by stacking multiple layers of perceptrons. This advancement, combined with effective training algorithms like backpropagation, paved the way for the deep learning revolution we see today.</p> <p>Understanding the perceptron model provides a foundation for grasping more complex neural network architectures used in modern radiation oncology applications, from contouring organs at risk to predicting treatment outcomes based on multidimensional data.</p> <h2 id="activation-functions">Activation Functions</h2> <p>Activation functions introduce non-linearity into neural networks, enabling them to learn complex patterns and relationships in data. Without activation functions, even a multi-layer neural network would behave like a single-layer linear model, regardless of its depth. This non-linearity is crucial for modeling the complex relationships present in medical data, such as the non-linear dose-response curves observed in radiation oncology.</p> <h3 id="sigmoid-tanh-relu-and-variants">Sigmoid, Tanh, ReLU and Variants</h3> <p>The sigmoid function, defined as σ(x) = 1/(1 + e^(-x)), maps input values to the range (0,1). This makes it interpretable as a probability, which is useful for binary classification problems. In early neural networks, sigmoid was a popular choice for hidden layer activations. However, it suffers from the “vanishing gradient problem” when used in deep networks—as inputs move away from zero, the gradient becomes extremely small, slowing down learning in earlier layers.</p> <p>The hyperbolic tangent (tanh) function is similar to sigmoid but maps inputs to the range (-1,1), making the outputs zero-centered. This property often leads to faster convergence during training compared to sigmoid. Like sigmoid, tanh also suffers from the vanishing gradient problem in deep networks.</p> <p>The Rectified Linear Unit (ReLU), defined as f(x) = max(0,x), has become the most widely used activation function in modern deep learning. ReLU simply outputs the input if it’s positive and zero otherwise. Its advantages include computational efficiency (requiring only a simple threshold operation) and reduced likelihood of vanishing gradients for positive inputs. However, ReLU can suffer from the “dying ReLU” problem, where neurons can become permanently inactive if they consistently receive negative inputs.</p> <p>Several variants of ReLU have been developed to address its limitations:</p> <p>Leaky ReLU allows a small gradient when the input is negative (f(x) = αx for x &lt; 0, where α is a small constant like 0.01), preventing neurons from “dying.”</p> <p>Parametric ReLU (PReLU) makes the slope for negative inputs a learnable parameter, allowing the model to determine the optimal value during training.</p> <p>Exponential Linear Unit (ELU) uses an exponential function for negative inputs, providing smoother transitions and potentially better performance in some applications.</p> <p>Swish, defined as f(x) = x * sigmoid(x), was discovered through automated search and has shown promising results in deep networks.</p> <p>In radiation oncology applications, the choice of activation function can significantly impact model performance. For instance, in dose prediction models, ReLU and its variants might be preferred for hidden layers due to their training efficiency, while sigmoid activations might be used in the output layer to constrain dose predictions to a reasonable range.</p> <h3 id="properties-and-use-cases">Properties and Use Cases</h3> <p>Different activation functions have distinct properties that make them suitable for specific use cases:</p> <p>Output range considerations: Sigmoid and tanh have bounded outputs, making them suitable for problems where predictions should fall within a specific range. In contrast, ReLU has an unbounded positive range, which can be advantageous for modeling quantities like radiation dose that cannot be negative but have no theoretical upper limit.</p> <p>Gradient behavior: The gradient of sigmoid and tanh approaches zero for inputs with large magnitude, potentially causing training to stall. ReLU has a constant gradient of 1 for positive inputs, facilitating more stable training in deep networks.</p> <p>Computational efficiency: ReLU and its variants are computationally efficient, requiring simple operations compared to the exponential calculations in sigmoid and tanh. This efficiency becomes significant when training large models on medical imaging data.</p> <p>Sparsity promotion: ReLU activations naturally induce sparsity in neural networks, as they output exactly zero for negative inputs. This sparsity can be beneficial for model regularization and interpretability, potentially important considerations in medical applications where understanding model behavior is crucial.</p> <p>In practice, modern deep learning architectures for radiation oncology applications typically use ReLU or its variants for hidden layers due to their training efficiency and effectiveness. For output layers, the activation function is chosen based on the specific task:</p> <p>Sigmoid for binary classification (e.g., tumor vs. normal tissue) Softmax for multi-class classification (e.g., multiple organ segmentation) Linear (no activation) for regression tasks (e.g., dose prediction) ReLU or similar for outputs that must be non-negative (e.g., dose-volume histograms)</p> <p>Understanding the properties of different activation functions helps in designing effective neural network architectures for specific radiation oncology applications and in diagnosing issues that may arise during training.</p> <h2 id="feedforward-neural-networks">Feedforward Neural Networks</h2> <p>Feedforward neural networks, also known as multi-layer perceptrons (MLPs), form the foundation of deep learning architectures. These networks consist of multiple layers of neurons, with information flowing in one direction from the input layer through one or more hidden layers to the output layer, without any cycles or loops.</p> <h3 id="architecture-and-layers">Architecture and Layers</h3> <p>The architecture of a feedforward neural network is defined by its layer structure:</p> <p>The input layer receives the raw features or data. In radiation oncology applications, these might include patient characteristics, dosimetric parameters, or flattened image data. The number of neurons in this layer corresponds to the dimensionality of the input data.</p> <p>Hidden layers perform intermediate computations and transformations. Each hidden layer consists of multiple neurons, with each neuron connected to all neurons in the previous layer (fully connected or dense layers). The number of hidden layers and neurons per layer are hyperparameters that significantly impact the network’s capacity and performance. Deeper networks (more layers) can learn more complex hierarchical representations but may be more difficult to train and prone to overfitting, especially with limited data.</p> <p>The output layer produces the final prediction or classification. Its structure depends on the specific task:</p> <ul> <li>For binary classification (e.g., malignant vs. benign), a single neuron with sigmoid activation is typically used.</li> <li>For multi-class classification (e.g., organ segmentation), multiple neurons with softmax activation provide class probabilities.</li> <li>For regression tasks (e.g., dose prediction), one or more neurons with linear or appropriate bounded activation functions output the predicted values.</li> </ul> <p>The connections between layers are represented by weight matrices, with each weight indicating the strength of the connection between two neurons. Additionally, each neuron (except in the input layer) has a bias term that allows the activation function to shift, providing greater flexibility in modeling.</p> <p>In radiation oncology, feedforward neural networks have been applied to various problems, including predicting treatment outcomes, estimating toxicity risks, and serving as building blocks for more complex architectures used in image analysis and treatment planning.</p> <h3 id="forward-and-backward-propagation">Forward and Backward Propagation</h3> <p>The operation of feedforward neural networks involves two key processes: forward propagation and backward propagation.</p> <p>Forward propagation is the process of computing the network’s output given an input. For each layer, the weighted sum of inputs from the previous layer is calculated, the bias term is added, and the result is passed through the activation function. This process continues layer by layer until reaching the output. Mathematically, for layer l:</p> <p>Z^(l) = W^(l)a^(l-1) + b^(l) a^(l) = f(Z^(l))</p> <p>Where W^(l) is the weight matrix, a^(l-1) is the activation from the previous layer, b^(l) is the bias vector, f is the activation function, Z^(l) is the weighted input, and a^(l) is the activation output.</p> <p>Backward propagation (backpropagation) is the process of computing gradients of the loss function with respect to the network parameters (weights and biases). These gradients indicate how to adjust the parameters to reduce the error. The process starts at the output layer by computing the error derivative with respect to the output, then works backward through the network using the chain rule of calculus.</p> <p>For the output layer, the gradient depends on the difference between predicted and actual values, modified by the derivative of the activation function. For hidden layers, the gradient depends on the weighted sum of gradients from the subsequent layer, again modified by the activation function derivative.</p> <p>Once gradients are computed, parameters are updated using an optimization algorithm like gradient descent:</p> <p>W^(l) = W^(l) - α * ∂L/∂W^(l) b^(l) = b^(l) - α * ∂L/∂b^(l)</p> <p>Where α is the learning rate and ∂L/∂W^(l) and ∂L/∂b^(l) are the gradients of the loss function with respect to the weights and biases.</p> <p>In radiation oncology applications, efficient and accurate backpropagation is crucial for training models that can reliably predict treatment outcomes or generate accurate contours. The complexity of the data and the critical nature of the predictions make proper training essential.</p> <h2 id="loss-functions">Loss Functions</h2> <p>Loss functions quantify the difference between a model’s predictions and the ground truth, providing a signal for how to adjust the model parameters during training. The choice of loss function significantly impacts what the model learns and how it performs on different types of errors.</p> <h3 id="mean-squared-error">Mean Squared Error</h3> <p>Mean Squared Error (MSE) is one of the most common loss functions for regression problems. It calculates the average of the squared differences between predicted and actual values:</p> <p>MSE = (1/n) * ∑(y_i - ŷ_i)²</p> <p>Where y_i is the true value, ŷ_i is the predicted value, and n is the number of samples.</p> <p>MSE heavily penalizes large errors due to the squaring operation, making it particularly sensitive to outliers. This property can be both an advantage and a disadvantage, depending on the application. In radiation oncology, MSE might be appropriate for dose prediction tasks where large deviations could have significant clinical consequences. However, its sensitivity to outliers could be problematic when working with noisy medical data.</p> <p>A variant of MSE is the Root Mean Squared Error (RMSE), which takes the square root of the MSE. This brings the error metric back to the same scale as the original data, making it more interpretable in the context of the specific problem.</p> <h3 id="cross-entropy">Cross-entropy</h3> <p>Cross-entropy loss is the standard choice for classification problems. For binary classification, it is defined as:</p> <p>BCE = -(1/n) * ∑[y_i * log(ŷ_i) + (1-y_i) * log(1-ŷ_i)]</p> <p>Where y_i is the true label (0 or 1), and ŷ_i is the predicted probability of class 1.</p> <p>For multi-class classification, categorical cross-entropy is used:</p> <p>CCE = -(1/n) * ∑∑[y_ij * log(ŷ_ij)]</p> <p>Where y_ij is 1 if sample i belongs to class j and 0 otherwise, and ŷ_ij is the predicted probability that sample i belongs to class j.</p> <p>Cross-entropy has several desirable properties for classification tasks. It heavily penalizes confident but wrong predictions, encouraging the model to output calibrated probabilities. It also produces larger gradients for misclassified examples compared to squared error, potentially leading to faster learning.</p> <p>In radiation oncology, cross-entropy is commonly used for classification tasks like tumor detection or organ segmentation (when framed as pixel-wise classification). However, for segmentation tasks, it’s often combined with other loss functions to address class imbalance issues, as we’ll discuss next.</p> <h3 id="focal-loss-and-specialized-functions">Focal Loss and Specialized Functions</h3> <p>Standard loss functions may not be optimal for all problems in radiation oncology. Specialized loss functions have been developed to address specific challenges:</p> <p>Focal Loss modifies cross-entropy to address class imbalance by down-weighting well-classified examples. It’s defined as:</p> <p>FL = -(1/n) * ∑[α * (1-ŷ_i)^γ * y_i * log(ŷ_i) + (1-α) * ŷ_i^γ * (1-y_i) * log(1-ŷ_i)]</p> <p>Where α balances the importance of positive/negative examples, and γ is a focusing parameter that reduces the loss contribution from easy examples. Focal loss is particularly useful in medical image segmentation where background pixels often vastly outnumber the pixels belonging to structures of interest.</p> <p>Dice Loss is based on the Dice similarity coefficient, a metric commonly used to evaluate segmentation quality. It’s defined as:</p> <p>DL = 1 - (2 * ∑(y_i * ŷ_i)) / (∑y_i + ∑ŷ_i)</p> <p>Dice loss directly optimizes for overlap between predicted and ground truth segmentations, making it well-suited for medical image segmentation tasks like organ contouring in radiation therapy planning.</p> <p>Hausdorff Distance-based losses incorporate distance metrics between predicted and ground truth boundaries. These are particularly relevant for radiation oncology, where the precise delineation of boundaries between tumors and organs at risk is critical for treatment planning.</p> <p>Boundary-aware losses place greater emphasis on accurately predicting the boundaries between different structures, which is crucial for precise contouring in radiation therapy.</p> <p>Combined losses often yield the best results for complex tasks. For instance, a weighted combination of cross-entropy and Dice loss can balance pixel-wise accuracy with overall segmentation quality. Similarly, adding regularization terms to the loss function can encourage desirable properties like smoothness in contours or dose distributions.</p> <p>In radiation oncology applications, the choice of loss function should be guided by clinical considerations. For example, in treatment planning, certain types of errors (like underdosing the tumor or overdosing critical structures) may have more severe consequences than others, suggesting the need for asymmetric loss functions that penalize these errors more heavily.</p> <h2 id="gradient-based-learning">Gradient-based Learning</h2> <p>Gradient-based learning forms the core of how neural networks are trained. By computing the gradient of the loss function with respect to the model parameters, these methods determine how to adjust the parameters to minimize the error.</p> <h3 id="backpropagation-algorithm">Backpropagation Algorithm</h3> <p>Backpropagation is the fundamental algorithm for efficiently computing gradients in neural networks. While we introduced it briefly earlier, let’s explore it in more detail:</p> <p>The algorithm consists of two main phases:</p> <ol> <li> <p>Forward pass: Input data is propagated through the network to compute predictions and the resulting loss.</p> </li> <li> <p>Backward pass: The gradient of the loss with respect to each parameter is computed by working backward from the output layer to the input layer.</p> </li> </ol> <p>The key insight of backpropagation is the efficient computation of these gradients using the chain rule of calculus. Rather than computing each gradient independently, intermediate results are cached and reused, dramatically reducing the computational cost.</p> <p>For a given layer l, the gradient of the loss L with respect to the weights W^(l) is:</p> <p>∂L/∂W^(l) = ∂L/∂Z^(l) * ∂Z^(l)/∂W^(l) = δ^(l) * (a^(l-1))^T</p> <p>Where δ^(l) = ∂L/∂Z^(l) is the “error” at layer l, and a^(l-1) is the activation from the previous layer.</p> <p>The error term δ^(l) is computed recursively:</p> <p>For the output layer: δ^(L) = ∂L/∂a^(L) ⊙ f’(Z^(L)) For hidden layers: δ^(l) = ((W^(l+1))^T * δ^(l+1)) ⊙ f’(Z^(l))</p> <p>Where ⊙ represents element-wise multiplication, and f’ is the derivative of the activation function.</p> <p>This recursive computation allows the error signal to propagate backward through the network, with each layer’s parameters receiving gradients that indicate how they contributed to the final error.</p> <p>In radiation oncology applications, backpropagation enables models to learn complex relationships between input data (like CT or MRI images) and output targets (like organ contours or dose distributions). The efficiency of backpropagation makes it practical to train deep networks on the large, high-dimensional datasets typical in medical imaging.</p> <h3 id="computational-graphs">Computational Graphs</h3> <p>Computational graphs provide a powerful framework for understanding and implementing backpropagation. A computational graph represents a mathematical expression as a directed graph where nodes are operations and edges represent data flowing between operations.</p> <p>For example, a simple neural network layer computing a = f(Wx + b) would be represented as a graph with nodes for matrix multiplication, addition, and the activation function, with edges showing how data flows through these operations.</p> <p>Computational graphs make the dependencies between variables explicit, facilitating the automatic computation of gradients. Modern deep learning frameworks like TensorFlow and PyTorch use computational graphs (either static or dynamic) to implement automatic differentiation, relieving developers from having to manually derive and implement gradient calculations.</p> <p>The forward pass through the graph computes the output values, while the backward pass computes gradients by applying the chain rule at each node. Each node knows how to compute the gradient of the output with respect to its inputs, given the gradient of the output with respect to its output.</p> <p>In complex models for radiation oncology applications, computational graphs can become quite large, with thousands or millions of operations. Automatic differentiation through these graphs enables the training of sophisticated models for tasks like multi-organ segmentation or dose prediction without requiring manual derivation of gradients.</p> <p>Understanding computational graphs also helps in diagnosing and addressing training issues, as it provides insight into how gradients flow through the network and where problems like vanishing or exploding gradients might occur.</p> <h2 id="initialization-strategies">Initialization Strategies</h2> <p>The initial values of neural network parameters significantly impact training dynamics and final performance. Poor initialization can lead to slow convergence, getting stuck in poor local minima, or even failure to train due to vanishing or exploding gradients.</p> <h3 id="random-initialization">Random Initialization</h3> <p>Simple random initialization from a uniform or normal distribution was used in early neural networks. However, this approach often leads to suboptimal training, especially in deep networks.</p> <p>Xavier/Glorot initialization, proposed by Xavier Glorot and Yoshua Bengio, draws weights from a distribution with variance scaled according to the number of input and output connections:</p> <p>Var(W) = 2 / (n_in + n_out)</p> <p>This scaling helps maintain the variance of activations and gradients across layers, preventing them from growing or shrinking exponentially with network depth.</p> <p>He initialization, developed by Kaiming He, modifies Xavier initialization for ReLU activations:</p> <p>Var(W) = 2 / n_in</p> <p>This accounts for the fact that ReLU activations effectively reduce the variance by setting negative values to zero.</p> <p>In radiation oncology applications, proper initialization is crucial for training deep networks on limited medical data. Good initialization can lead to faster convergence and better final performance, which is particularly important when working with the complex, high-dimensional data typical in medical imaging.</p> <h3 id="specialized-initialization-methods">Specialized Initialization Methods</h3> <p>Beyond standard methods, specialized initialization strategies have been developed for specific architectures and problems:</p> <p>Orthogonal initialization sets weight matrices to be orthogonal, which helps preserve gradient magnitudes during backpropagation. This can be particularly useful in very deep networks or recurrent architectures.</p> <p>Identity initialization, where weight matrices start close to the identity matrix, has shown benefits in residual networks by initially preserving the input features and allowing the network to gradually learn transformations.</p> <p>Pretrained initialization uses weights from models trained on related tasks or larger datasets. This transfer learning approach is especially valuable in medical imaging, where labeled data may be limited. For example, a segmentation model for radiation therapy planning might initialize with weights from a network pretrained on general medical image segmentation tasks.</p> <p>In radiation oncology applications, the choice of initialization strategy should consider the network architecture, activation functions, and the specific characteristics of the data. Proper initialization can help models converge to better solutions, particularly when working with the limited datasets often available in medical applications.</p> <h2 id="batch-normalization">Batch Normalization</h2> <p>Batch Normalization (BatchNorm) is a technique that normalizes the inputs to each layer, stabilizing and accelerating training. Introduced by Sergey Ioffe and Christian Szegedy in 2015, it has become a standard component in many deep learning architectures.</p> <h3 id="how-batch-normalization-works">How Batch Normalization Works</h3> <p>BatchNorm normalizes the pre-activation values of a layer by subtracting the batch mean and dividing by the batch standard deviation:</p> <p>x̂ = (x - μ_B) / √(σ_B² + ε)</p> <p>Where μ_B and σ_B² are the mean and variance of the current mini-batch, and ε is a small constant for numerical stability.</p> <p>After normalization, BatchNorm applies a learnable scale and shift:</p> <p>y = γ * x̂ + β</p> <p>Where γ and β are learnable parameters that allow the network to undo the normalization if necessary.</p> <p>During training, BatchNorm uses mini-batch statistics for normalization. During inference, it uses running estimates of the population mean and variance accumulated during training.</p> <h3 id="benefits-of-batch-normalization">Benefits of Batch Normalization</h3> <p>BatchNorm offers several advantages that make it particularly valuable for training deep networks:</p> <p>Reduced internal covariate shift: By normalizing layer inputs, BatchNorm reduces the changes in distribution that hidden layers experience as earlier layers update during training. This stabilizes the learning process.</p> <p>Faster training: BatchNorm often allows the use of higher learning rates and reduces the number of epochs required for convergence.</p> <p>Regularization effect: The noise introduced by estimating statistics from mini-batches acts as a form of regularization, potentially reducing the need for dropout or weight decay.</p> <p>Reduced sensitivity to initialization: BatchNorm makes networks less sensitive to the choice of initialization scheme, as it normalizes the activations regardless of their initial scale.</p> <p>In radiation oncology applications, these benefits can be particularly valuable when training complex models on limited data. The stabilizing effect of BatchNorm can help models converge to better solutions, while its regularization effect can reduce overfitting on small medical datasets.</p> <h3 id="considerations-for-medical-applications">Considerations for Medical Applications</h3> <p>While BatchNorm is widely used, there are some considerations specific to medical applications:</p> <p>Batch size dependency: BatchNorm’s performance depends on having reasonably sized mini-batches to estimate statistics. In medical imaging, where high-resolution 3D images may limit batch sizes due to memory constraints, alternatives like Group Normalization or Instance Normalization might be more appropriate.</p> <p>Domain shift: When applying models to data from different institutions or scanners, the statistics used by BatchNorm may no longer be appropriate. Techniques like Adaptive BatchNorm or Domain Adaptation methods may be needed to address this issue.</p> <p>Inference consistency: In clinical applications where reproducibility is crucial, the stochasticity introduced by BatchNorm during training needs to be carefully managed to ensure consistent inference results.</p> <p>Despite these considerations, BatchNorm or its variants are commonly used in deep learning models for radiation oncology applications, contributing to more stable and efficient training of complex architectures for tasks like organ segmentation and dose prediction.</p> <h2 id="convolutional-neural-networks-cnns">Convolutional Neural Networks (CNNs)</h2> <p>Convolutional Neural Networks (CNNs) have revolutionized the field of computer vision and medical image analysis. These specialized neural network architectures are designed to process data with grid-like topology, such as images, by leveraging spatial relationships between pixels. In radiation oncology, CNNs have become the backbone of many automated contouring and image analysis systems.</p> <h3 id="convolutional-layers-and-operations">Convolutional Layers and Operations</h3> <p>The convolutional layer is the core building block of CNNs. Unlike fully connected layers that connect each neuron to every neuron in the previous layer, convolutional layers use a set of learnable filters (or kernels) that slide across the input, computing dot products between the filter weights and the input values at each spatial position. This operation, called convolution, can be mathematically represented as:</p> <p>(I * K)(x, y) = ∑∑ I(x-m, y-n) K(m, n)</p> <p>Where I is the input, K is the kernel, and (x, y) represents spatial coordinates.</p> <p>Each filter in a convolutional layer detects specific patterns or features, such as edges, textures, or more complex structures in deeper layers. The key properties that make convolutional layers particularly effective for image processing include:</p> <p>Parameter sharing: The same filter weights are applied across the entire input, dramatically reducing the number of parameters compared to fully connected layers. This makes CNNs more efficient and less prone to overfitting, especially important when working with limited medical imaging datasets.</p> <p>Local connectivity: Each neuron connects only to a small region of the input (the receptive field), reflecting the local nature of many image features. This mimics how the human visual system processes information and helps the network focus on relevant local patterns.</p> <p>Translation invariance: Features can be detected regardless of their position in the image, making CNNs robust to spatial translations of the input. This is particularly valuable in medical imaging, where anatomical structures may appear in different locations across patients.</p> <p>In radiation oncology applications, convolutional layers can learn to identify relevant anatomical structures, tumor boundaries, and tissue characteristics from CT, MRI, or PET images. The hierarchical feature learning in CNNs—from simple edges in early layers to complex anatomical patterns in deeper layers—aligns well with the multi-scale nature of medical image analysis.</p> <h3 id="pooling-layers">Pooling Layers</h3> <p>Pooling layers reduce the spatial dimensions of feature maps, providing several benefits:</p> <p>Dimensionality reduction: By downsampling the feature maps, pooling reduces the computational load and memory requirements of the network. This is particularly important when working with high-resolution medical images.</p> <p>Translation invariance: Pooling increases the network’s robustness to small translations or distortions in the input, as the exact location of a feature becomes less important after pooling.</p> <p>Increasing receptive field: By reducing spatial dimensions, pooling allows neurons in subsequent layers to “see” a larger portion of the original input, helping the network capture larger-scale patterns.</p> <p>The most common pooling operations include:</p> <p>Max pooling: Takes the maximum value within each pooling window, effectively selecting the strongest feature response. This is the most widely used pooling operation due to its simplicity and effectiveness.</p> <p>Average pooling: Computes the average value within each pooling window, preserving more information but potentially diluting strong feature activations.</p> <p>In modern CNN architectures for medical imaging, max pooling is commonly used after convolutional layers to progressively reduce spatial dimensions while preserving the most salient features. However, in segmentation networks like U-Net, which are widely used in radiation oncology for contouring, the spatial information lost during pooling must be recovered through upsampling or transposed convolutions in the decoder path.</p> <h3 id="cnn-architectures">CNN Architectures</h3> <p>The field of CNN architecture design has evolved rapidly, with numerous influential models that have progressively improved performance on image analysis tasks:</p> <p>LeNet, developed by Yann LeCun in the late 1990s, was one of the earliest CNN architectures, designed for handwritten digit recognition. It established the basic pattern of alternating convolutional and pooling layers followed by fully connected layers.</p> <p>AlexNet, which won the ImageNet competition in 2012, marked the beginning of the deep learning revolution in computer vision. It featured deeper layers, ReLU activations, and techniques like dropout and data augmentation to prevent overfitting.</p> <p>VGG networks, introduced in 2014, used very small (3×3) convolutional filters throughout the network, showing that depth was more important than filter size for learning complex features. The simplicity and uniformity of VGG’s design made it a popular choice for transfer learning in medical imaging.</p> <p>ResNet (Residual Network) addressed the degradation problem in very deep networks by introducing skip connections that allow gradients to flow more easily during backpropagation. These residual connections enable the training of networks with hundreds of layers, significantly increasing model capacity without suffering from vanishing gradients.</p> <p>DenseNet took the concept of skip connections further by connecting each layer to every other layer in a feed-forward fashion. This dense connectivity pattern encourages feature reuse and improves gradient flow, resulting in more efficient parameter usage.</p> <p>EfficientNet optimized both network depth and width using a compound scaling method, achieving state-of-the-art performance with fewer parameters. This efficiency is particularly valuable in medical applications where computational resources may be limited.</p> <p>In radiation oncology, these architectures have been adapted for tasks like tumor detection, organ segmentation, and treatment response prediction. The choice of architecture depends on factors like the specific task, available data, computational resources, and the need for real-time performance in clinical settings.</p> <h3 id="transfer-learning-with-cnns">Transfer Learning with CNNs</h3> <p>Transfer learning leverages knowledge gained from one task to improve performance on another, related task. In the context of CNNs, this typically involves:</p> <ol> <li>Pretraining a network on a large dataset (like ImageNet) where data is abundant</li> <li>Transferring the learned weights to a new network for the target task</li> <li>Fine-tuning some or all of the weights on the target dataset</li> </ol> <p>This approach is particularly valuable in medical imaging, where labeled data is often scarce. By starting with weights pretrained on natural images, models can leverage general visual features (edges, textures, shapes) that transfer well to medical images, despite the domain difference.</p> <p>Several transfer learning strategies exist:</p> <p>Feature extraction: The pretrained network is used as a fixed feature extractor, with only the final classification layers trained on the new task. This approach works well when the target dataset is small and similar to the pretraining dataset.</p> <p>Fine-tuning: Some or all of the pretrained weights are updated during training on the new task. Typically, earlier layers (which capture more generic features) are frozen or updated with a smaller learning rate, while later layers (which capture more task-specific features) are fully fine-tuned.</p> <p>Domain adaptation: Specific techniques are applied to address the domain shift between the pretraining data (e.g., natural images) and the target data (e.g., medical images). These might include adversarial training or specific loss functions designed to align feature distributions.</p> <p>In radiation oncology, transfer learning has been successfully applied to various tasks, including organ segmentation, tumor classification, and treatment response prediction. For example, a CNN pretrained on natural images might be fine-tuned to segment organs at risk in CT scans, with the early layers capturing general image features and the later layers adapting to the specific characteristics of CT imaging and anatomical structures.</p> <p>The effectiveness of transfer learning in medical imaging depends on several factors:</p> <p>The similarity between the source and target domains The amount of available target data The complexity of the target task The architecture of the pretrained model</p> <p>When properly applied, transfer learning can significantly reduce the amount of labeled data needed for training, accelerate convergence, and improve final performance—all crucial advantages in the data-limited domain of radiation oncology.</p> <h2 id="recurrent-neural-networks-rnns">Recurrent Neural Networks (RNNs)</h2> <p>While CNNs excel at processing spatial data like images, Recurrent Neural Networks (RNNs) are designed for sequential data, where the order of elements matters. In radiation oncology, RNNs can be valuable for analyzing temporal data such as treatment response over time, patient monitoring during treatment courses, or even the sequential processing of 3D volumes slice by slice.</p> <h3 id="sequential-data-processing">Sequential Data Processing</h3> <p>RNNs process sequential data by maintaining an internal state (or “memory”) that captures information from previous time steps. At each step, the network takes both the current input and its previous state to produce an output and update the state. This recurrent connection allows information to persist across the sequence.</p> <p>The basic RNN computation can be expressed as:</p> <p>h_t = f(W_xh * x_t + W_hh * h_{t-1} + b_h) y_t = g(W_hy * h_t + b_y)</p> <p>Where:</p> <ul> <li>x_t is the input at time step t</li> <li>h_t is the hidden state at time step t</li> <li>y_t is the output at time step t</li> <li>W_xh, W_hh, W_hy are weight matrices</li> <li>b_h, b_y are bias vectors</li> <li>f and g are activation functions</li> </ul> <p>This recurrent structure enables RNNs to model dependencies between elements in a sequence, making them suitable for tasks like:</p> <p>Time series prediction: Forecasting future values based on past observations, such as predicting tumor response based on sequential imaging or biomarker measurements.</p> <p>Sequence classification: Assigning a label to an entire sequence, such as classifying a patient’s treatment trajectory as responding or non-responding.</p> <p>Sequence generation: Producing sequential outputs, such as generating synthetic treatment plans or patient trajectories for simulation.</p> <p>In radiation oncology, sequential data appears in various forms: the progression of tumor response over a treatment course, longitudinal patient monitoring data, or even the spatial progression through slices of a 3D volume when computational constraints prevent processing the entire volume at once.</p> <h3 id="vanishingexploding-gradients">Vanishing/Exploding Gradients</h3> <p>Despite their theoretical ability to capture long-range dependencies, basic RNNs suffer from the vanishing and exploding gradient problems during training:</p> <p>Vanishing gradients occur when the gradients become extremely small as they’re propagated back through time steps, effectively preventing the network from learning long-range dependencies. This happens because the repeated multiplication of small values (less than 1) during backpropagation causes gradients to decay exponentially.</p> <p>Exploding gradients represent the opposite problem, where gradients grow exponentially large, causing unstable training and parameter updates that overshoot optimal values. This typically occurs when weights are large (greater than 1).</p> <p>These issues are particularly problematic for long sequences, as the effects compound with each time step. In radiation oncology applications, this could limit the ability to model long-term dependencies in treatment response or patient monitoring data spanning many weeks or months.</p> <p>Several techniques have been developed to address these issues:</p> <p>Gradient clipping prevents exploding gradients by scaling gradients when their norm exceeds a threshold.</p> <p>Careful weight initialization helps establish initial conditions that mitigate both vanishing and exploding gradients.</p> <p>Skip connections or residual connections provide shortcuts for gradient flow, similar to their use in deep CNNs.</p> <p>However, the most effective solution has been the development of specialized RNN architectures like LSTM and GRU, which we’ll discuss next.</p> <h3 id="lstm-and-gru-architectures">LSTM and GRU Architectures</h3> <p>Long Short-Term Memory (LSTM) networks were designed specifically to address the vanishing gradient problem in RNNs. LSTMs introduce a more complex cell structure with three gates that regulate information flow:</p> <p>The forget gate determines what information to discard from the cell state. The input gate decides what new information to store in the cell state. The output gate controls what parts of the cell state to output.</p> <p>This gating mechanism allows LSTMs to preserve information over many time steps when needed, while also being able to update or forget information when appropriate. Mathematically, the LSTM operations can be expressed as:</p> <p>f_t = σ(W_f * [h_{t-1}, x_t] + b_f) # Forget gate i_t = σ(W_i * [h_{t-1}, x_t] + b_i) # Input gate o_t = σ(W_o * [h_{t-1}, x_t] + b_o) # Output gate c̃<em>t = tanh(W_c * [h</em>{t-1}, x_t] + b_c) # Candidate cell state c_t = f_t * c_{t-1} + i_t * c̃_t # Cell state update h_t = o_t * tanh(c_t) # Hidden state output</p> <p>Where σ is the sigmoid function, * represents element-wise multiplication, and [h_{t-1}, x_t] denotes concatenation.</p> <p>Gated Recurrent Units (GRUs) are a simplified variant of LSTMs with fewer parameters. GRUs combine the forget and input gates into a single “update gate” and merge the cell state and hidden state. This simplification makes GRUs computationally more efficient while often achieving comparable performance to LSTMs.</p> <p>In radiation oncology applications, LSTMs and GRUs can model complex temporal patterns in treatment response, capture long-term dependencies in patient monitoring data, or process 3D volumes slice by slice while maintaining spatial context across slices. For example, an LSTM might analyze a sequence of tumor measurements during treatment to predict the final response or to identify patients who might benefit from treatment adaptation.</p> <p>The choice between LSTM and GRU often depends on the specific application, with GRUs being more efficient but LSTMs potentially offering more modeling capacity for complex sequences. In practice, both architectures significantly outperform basic RNNs for most tasks involving long-range dependencies.</p> <h2 id="autoencoders">Autoencoders</h2> <p>Autoencoders are neural networks designed to learn efficient representations of data in an unsupervised manner. Their architecture consists of an encoder that compresses the input into a lower-dimensional latent space and a decoder that reconstructs the original input from this compressed representation. This structure makes autoencoders particularly useful for dimensionality reduction, feature learning, and generative modeling.</p> <h3 id="dimensionality-reduction">Dimensionality Reduction</h3> <p>One of the primary applications of autoencoders is dimensionality reduction—transforming high-dimensional data into a lower-dimensional representation while preserving essential information. Unlike traditional methods like Principal Component Analysis (PCA), which is limited to linear transformations, autoencoders can learn non-linear mappings, potentially capturing more complex relationships in the data.</p> <p>The encoder portion of an autoencoder compresses the input x into a latent representation z:</p> <p>z = f_encoder(x)</p> <p>Where f_encoder typically consists of multiple neural network layers with progressively fewer neurons, culminating in the bottleneck layer that represents the latent space.</p> <p>In radiation oncology, dimensionality reduction through autoencoders can be valuable for:</p> <p>Compressing high-dimensional medical images or dose distributions into more manageable representations for subsequent analysis or modeling.</p> <p>Extracting meaningful features from complex, multi-modal data sources like combined CT, MRI, and PET images.</p> <p>Visualizing high-dimensional patient data in lower dimensions to identify patterns or clusters that might inform treatment strategies.</p> <p>Reducing the dimensionality of input data for other machine learning models, potentially improving their performance when training data is limited.</p> <p>The effectiveness of an autoencoder for dimensionality reduction depends on its architecture (depth, width, activation functions), the dimensionality of the latent space, and the training procedure. The goal is to find a balance where the latent representation is compact enough to be useful but still retains the information necessary for the task at hand.</p> <h3 id="denoising-autoencoders">Denoising Autoencoders</h3> <p>Denoising autoencoders (DAEs) are a variant designed to learn robust representations by reconstructing clean inputs from corrupted versions. During training, noise is deliberately added to the input, and the network learns to recover the original, uncorrupted data:</p> <p>x̃ = corrupt(x) # Add noise to input z = f_encoder(x̃) # Encode corrupted input x’ = f_decoder(z) # Reconstruct original input Loss = ||x - x’||² # Compare reconstruction to original</p> <p>This process forces the network to learn features that are robust to variations and noise in the input, rather than simply learning to copy the input to the output.</p> <p>In medical imaging applications, denoising autoencoders can:</p> <p>Improve image quality by removing noise from low-dose CT scans, potentially enabling dose reduction without sacrificing diagnostic quality.</p> <p>Learn features that are invariant to common artifacts or variations in imaging protocols, improving the robustness of subsequent analysis.</p> <p>Serve as a preprocessing step for other deep learning models, providing cleaner inputs that may lead to better performance.</p> <p>The concept of denoising can be extended beyond simple noise removal to handling other types of corruption or variation, such as missing data, intensity variations between scanners, or motion artifacts—all common challenges in medical imaging.</p> <h3 id="variational-autoencoders-vaes">Variational Autoencoders (VAEs)</h3> <p>Variational Autoencoders (VAEs) extend the autoencoder framework to learn not just a compressed representation but a probabilistic latent space. Instead of encoding an input as a single point in the latent space, VAEs encode it as a probability distribution, typically a Gaussian with learned mean and variance parameters.</p> <p>The encoder in a VAE outputs parameters of this distribution:</p> <p>μ, σ = f_encoder(x)</p> <p>A sample is then drawn from this distribution using the reparameterization trick to allow gradient-based training:</p> <p>z = μ + σ * ε, where ε ~ N(0, 1)</p> <p>The decoder then reconstructs the input from this sampled latent vector:</p> <p>x’ = f_decoder(z)</p> <p>VAEs are trained with a composite loss function that balances reconstruction quality against the regularization of the latent space:</p> <p>Loss = Reconstruction_Loss + KL_Divergence</p> <p>The Kullback-Leibler divergence term encourages the learned latent distributions to be close to a standard normal distribution, creating a continuous, structured latent space that facilitates generation and interpolation.</p> <p>In radiation oncology, VAEs offer several unique capabilities:</p> <p>Generating synthetic medical images or dose distributions by sampling from the latent space, potentially useful for data augmentation or simulation.</p> <p>Interpolating between different patients or treatment plans in the latent space to explore the continuum of possible variations.</p> <p>Anomaly detection by identifying inputs that have high reconstruction error or that map to low-probability regions of the latent space, potentially flagging unusual anatomical configurations or treatment plans for review.</p> <p>Disentangled representation learning, where different dimensions of the latent space capture independent factors of variation in the data, such as separating anatomical variations from pathological changes.</p> <p>The probabilistic nature of VAEs makes them particularly suitable for medical applications where quantifying uncertainty is important. By generating multiple reconstructions or predictions through repeated sampling from the latent distribution, VAEs can provide a measure of confidence or variability in their outputs.</p> <h2 id="generative-adversarial-networks-gans">Generative Adversarial Networks (GANs)</h2> <p>Generative Adversarial Networks (GANs) represent a powerful framework for generative modeling, capable of producing remarkably realistic synthetic data. Introduced by Ian Goodfellow in 2014, GANs consist of two neural networks—a generator and a discriminator—trained in an adversarial process that drives both to improve.</p> <h3 id="generator-and-discriminator">Generator and Discriminator</h3> <p>The generator network G takes random noise z as input and produces synthetic data G(z) that aims to mimic the distribution of real data. The generator never sees the real data directly; it learns solely from the feedback provided by the discriminator.</p> <p>The discriminator network D acts as a binary classifier, taking either real data x or generated data G(z) as input and outputting a probability that the input comes from the real data distribution rather than the generator. The discriminator’s goal is to correctly distinguish between real and generated samples.</p> <p>These two networks are trained simultaneously in a minimax game:</p> <p>min_G max_D V(D, G) = E_x~p_data [log D(x)] + E_z~p_z [log(1 - D(G(z)))]</p> <p>Where:</p> <ul> <li>The discriminator D tries to maximize V by correctly classifying real and fake samples</li> <li>The generator G tries to minimize V by producing samples that fool the discriminator</li> </ul> <p>In radiation oncology applications, the generator might produce synthetic medical images, contours, or dose distributions, while the discriminator learns to distinguish these from real clinical data. As training progresses, the generator produces increasingly realistic outputs that capture the complex patterns and relationships present in the training data.</p> <h3 id="training-dynamics">Training Dynamics</h3> <p>Training GANs is notoriously challenging due to several issues:</p> <p>Mode collapse occurs when the generator produces limited varieties of outputs, failing to capture the full diversity of the real data distribution. In medical applications, this might manifest as generating only the most common anatomical configurations while failing to represent rarer but clinically important variations.</p> <p>Training instability arises from the adversarial nature of the training process. If one network becomes too powerful relative to the other, learning can stall or diverge. For example, if the discriminator becomes too effective too quickly, the generator may receive insufficient gradient information to improve.</p> <p>Vanishing gradients can occur when the discriminator becomes too confident, providing minimal useful feedback to the generator. Conversely, if the generator consistently fools the discriminator, the latter may receive insufficient signal to improve.</p> <p>Several techniques have been developed to address these challenges:</p> <p>Wasserstein GAN (WGAN) replaces the original GAN objective with the Wasserstein distance, which provides more stable gradients throughout training.</p> <p>Spectral normalization constrains the discriminator’s capacity by normalizing its weights, helping maintain balance between the networks.</p> <p>Progressive growing starts with low-resolution images and gradually increases resolution during training, allowing the networks to learn large-scale structure before fine details.</p> <p>In medical applications, where data quality and diversity are paramount, addressing these training challenges is crucial for developing GANs that can generate clinically useful synthetic data.</p> <h3 id="applications-in-image-synthesis">Applications in Image Synthesis</h3> <p>GANs have numerous applications in radiation oncology and medical imaging:</p> <p>Synthetic data generation can augment limited datasets, helping to train more robust models for tasks like organ segmentation or treatment planning. For example, a GAN might generate additional variations of rare anatomical configurations to ensure models can handle these cases.</p> <p>Image-to-image translation enables transforming images from one domain to another, such as converting MRI to synthetic CT for treatment planning when actual CT is unavailable, or translating between different MRI sequences.</p> <p>Super-resolution techniques can enhance the quality of low-resolution medical images, potentially improving diagnostic accuracy or enabling more precise contouring.</p> <p>Cross-modality synthesis can generate missing imaging modalities based on available ones, such as creating synthetic PET images from CT scans, potentially reducing the need for multiple scans.</p> <p>Anomaly detection leverages the GAN’s learned representation of normal anatomy to identify abnormalities or pathologies as deviations from this norm.</p> <p>Domain adaptation uses GANs to align feature distributions between source and target domains, helping models trained on data from one institution or scanner generalize to others.</p> <p>The quality and utility of GAN-generated images in clinical applications depend on several factors:</p> <p>Training data quality and diversity Network architecture and capacity Training stability and convergence Evaluation metrics that capture clinically relevant aspects of image quality</p> <p>While GANs show tremendous promise for medical image synthesis, their deployment in clinical settings requires careful validation to ensure the generated images preserve the clinically relevant features of the original data and don’t introduce artifacts that could affect diagnosis or treatment planning.</p> <h2 id="transformers-and-attention-mechanisms">Transformers and Attention Mechanisms</h2> <p>Transformers have revolutionized natural language processing and are increasingly being applied to computer vision and medical image analysis. Unlike CNNs and RNNs, which process data sequentially or locally, transformers process all elements of the input simultaneously through self-attention mechanisms, capturing long-range dependencies more effectively.</p> <h3 id="self-attention">Self-attention</h3> <p>Self-attention is the core mechanism that allows transformers to weigh the importance of different elements in the input when processing each element. For each position in the input sequence, self-attention computes a weighted sum of all positions, with weights determined by the similarity between elements.</p> <p>The standard self-attention mechanism, known as “scaled dot-product attention,” is computed as:</p> <p>Attention(Q, K, V) = softmax(QK^T / √d_k)V</p> <p>Where:</p> <ul> <li>Q (queries), K (keys), and V (values) are linear projections of the input</li> <li>d_k is the dimension of the keys, used for scaling to prevent extremely small gradients</li> <li>The softmax function normalizes the attention weights</li> </ul> <p>In medical imaging applications, self-attention allows the model to focus on relevant regions of an image when processing each location. For example, when segmenting a tumor, the model can attend to similar-appearing regions elsewhere in the image or to anatomical landmarks that provide context, even if they’re distant from the current location.</p> <p>The global receptive field of self-attention—each position can attend to all other positions—contrasts with the limited receptive field of convolutional operations, which only consider local neighborhoods. This global context is particularly valuable in medical imaging, where distant anatomical relationships often provide important diagnostic or segmentation cues.</p> <h3 id="multi-head-attention">Multi-head Attention</h3> <p>Multi-head attention extends self-attention by running multiple attention operations in parallel, each with different learned projections. This allows the model to jointly attend to information from different representation subspaces and at different positions:</p> <p>MultiHead(Q, K, V) = Concat(head_1, …, head_h)W^O where head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)</p> <p>Where W_i^Q, W_i^K, W_i^V, and W^O are learnable parameter matrices.</p> <p>In medical imaging, multi-head attention enables the model to simultaneously capture different types of relationships:</p> <p>One head might focus on texture similarities Another might attend to shape correspondences A third might emphasize anatomical positioning</p> <p>This multi-faceted attention helps the model integrate various aspects of the image when making predictions, potentially leading to more accurate and robust performance on complex tasks like organ segmentation or tumor classification.</p> <h3 id="transformer-architecture">Transformer Architecture</h3> <p>The complete transformer architecture consists of several key components:</p> <p>Input embeddings convert the raw input (whether text tokens or image patches) into a continuous vector representation. In vision transformers, images are typically divided into fixed-size patches, each embedded as a vector.</p> <p>Positional encodings add information about the position of each element, since self-attention itself is permutation-invariant. These encodings can be fixed sinusoidal functions or learned parameters.</p> <p>The encoder stack consists of multiple identical layers, each containing:</p> <ul> <li>A multi-head self-attention mechanism</li> <li>A position-wise feed-forward network</li> <li>Layer normalization and residual connections</li> </ul> <p>The decoder stack (used in sequence generation tasks) has a similar structure but includes an additional cross-attention mechanism that attends to the encoder’s output.</p> <p>In medical imaging applications, transformers have been adapted in several ways:</p> <p>Vision Transformer (ViT) divides images into patches and processes them as a sequence, demonstrating competitive performance with CNNs on image classification tasks.</p> <p>UNETR (UNet Transformer) combines the U-Net architecture popular in medical segmentation with transformer encoders, leveraging both local and global context.</p> <p>Swin Transformer uses shifted windows to efficiently compute self-attention locally while allowing for cross-window connections, addressing the computational challenges of applying self-attention to high-resolution images.</p> <p>These transformer-based architectures have shown promising results in radiation oncology applications, including organ segmentation, tumor detection, and treatment response prediction. Their ability to capture long-range dependencies complements the local pattern recognition strengths of CNNs, leading to hybrid approaches that combine the best of both worlds.</p> <h2 id="u-net-and-segmentation-architectures">U-Net and Segmentation Architectures</h2> <p>Segmentation—the task of assigning a class label to each pixel or voxel in an image—is fundamental in radiation oncology for delineating tumors, organs at risk, and other anatomical structures. U-Net and its variants have become the dominant architectural paradigm for medical image segmentation due to their effectiveness in preserving both local and global context.</p> <h3 id="encoder-decoder-structures">Encoder-decoder Structures</h3> <p>Encoder-decoder architectures for segmentation typically follow a pattern of:</p> <ol> <li>An encoder path that progressively reduces spatial dimensions while increasing feature channels, capturing increasingly abstract representations</li> <li>A decoder path that recovers spatial resolution while decreasing feature channels, generating the segmentation map</li> </ol> <p>This structure allows the network to:</p> <p>Capture hierarchical features at multiple scales in the encoder Generate detailed, pixel-level predictions in the decoder Maintain a reasonable parameter count by working with reduced spatial dimensions in the intermediate layers</p> <p>The U-Net architecture, introduced by Ronneberger et al. in 2015 specifically for biomedical image segmentation, follows this encoder-decoder pattern but adds a crucial innovation: skip connections.</p> <h3 id="skip-connections">Skip Connections</h3> <p>Skip connections directly connect layers in the encoder path to corresponding layers in the decoder path, allowing the decoder to access features from earlier layers. These connections serve several important purposes:</p> <p>Preserving spatial information: While the encoder captures increasingly abstract features, it loses spatial precision due to pooling operations. Skip connections provide the decoder with access to the higher-resolution features from the encoder, helping generate more precise segmentation boundaries.</p> <p>Mitigating the vanishing gradient problem: By providing shorter paths for gradient flow during backpropagation, skip connections help train deeper networks more effectively.</p> <p>Combining multi-scale information: Skip connections allow the network to fuse features at different scales, capturing both fine details and broader contextual information.</p> <p>In the original U-Net, skip connections are implemented as concatenations, where feature maps from the encoder are concatenated with the corresponding decoder features along the channel dimension. This allows the decoder to selectively use information from both paths.</p> <p>The effectiveness of skip connections has made them a standard component in segmentation architectures, with variations appearing in numerous U-Net derivatives and other segmentation networks.</p> <h3 id="specialized-architectures-for-medical-imaging">Specialized Architectures for Medical Imaging</h3> <p>Building on the U-Net foundation, numerous specialized architectures have been developed to address the unique challenges of medical image segmentation:</p> <p>V-Net extends U-Net to 3D volumes, using 3D convolutions throughout the network to process volumetric medical data like CT or MRI scans. This allows the network to leverage information across slices, capturing 3D anatomical relationships that might be missed when processing 2D slices independently.</p> <p>Attention U-Net incorporates attention gates that help the model focus on relevant regions, suppressing irrelevant responses in feature maps. This is particularly valuable in medical imaging, where the structures of interest may occupy only a small portion of the image.</p> <p>nnU-Net (no new U-Net) is not a single architecture but a self-configuring framework that automatically adapts the U-Net design to the specific characteristics of a dataset, including image size, spacing, and modality. It has achieved state-of-the-art results across diverse medical segmentation tasks by optimizing preprocessing, network architecture, training, and post-processing.</p> <p>TransUNet combines transformers with U-Net, using a transformer encoder to capture global context and a convolutional decoder with skip connections to generate detailed segmentations. This hybrid approach leverages the strengths of both transformers (global relationships) and CNNs (local patterns).</p> <p>In radiation oncology, these specialized architectures have been applied to various segmentation tasks:</p> <p>Multi-organ segmentation for treatment planning, identifying organs at risk to avoid during radiation delivery Tumor delineation, precisely defining the target volume for treatment Adaptive radiotherapy, tracking changes in anatomy over the course of treatment Automatic contouring for routine structures, reducing the clinical workload</p> <p>The choice of architecture depends on factors like the specific segmentation task, available computational resources, dataset characteristics, and required inference speed for clinical workflow integration.</p> <h3 id="further-reading">Further Reading</h3> <p><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC5959832">Hello World: Deep Learning in Medical Imaging</a> is a dated but useful starting point for medical image classification using a standard off-the-shelf deep neural network architecture.</p>]]></content><author><name></name></author><category term="tairo,"/><category term="radiation-oncology,"/><category term="artificial-intelligence"/><summary type="html"><![CDATA[2: Fundamentals of Artificial Intelligence through Deep Learning]]></summary></entry><entry><title type="html">TaiRO: Introduction</title><link href="https://amithjkamath.github.io/blog/2025/01-TaiRO-Intro/" rel="alternate" type="text/html" title="TaiRO: Introduction"/><published>2025-01-01T00:00:00+00:00</published><updated>2025-01-01T00:00:00+00:00</updated><id>https://amithjkamath.github.io/blog/2025/01-TaiRO-Intro</id><content type="html" xml:base="https://amithjkamath.github.io/blog/2025/01-TaiRO-Intro/"><![CDATA[<h1 id="1-introduction">1: Introduction</h1> <h2 id="historical-context-and-evolution-of-ai">Historical Context and Evolution of AI</h2> <p>Artificial Intelligence as a field has roots dating back to the 1950s, but the concepts that would eventually lead to modern AI began much earlier with philosophical questions about the nature of knowledge, reasoning, and the possibility of creating thinking machines. The formal birth of AI as a discipline is often attributed to the Dartmouth Conference of 1956, where John McCarthy, Marvin Minsky, Claude Shannon, and others gathered to discuss the possibility of creating machines that could “think.”</p> <p>The early decades of AI research were characterized by periods of great optimism followed by “AI winters” when progress slowed and funding decreased. Early AI approaches focused on symbolic reasoning and rule-based systems, attempting to encode human knowledge explicitly. These systems showed promise in narrow domains but struggled with the complexity and ambiguity of real-world problems.</p> <p>The evolution of AI has been marked by several paradigm shifts. From the rule-based expert systems of the 1970s and 1980s to the statistical approaches that gained prominence in the 1990s, each era brought new insights and techniques. The current deep learning revolution, which began in earnest around 2012 with breakthroughs in image recognition using convolutional neural networks, represents perhaps the most significant shift yet.</p> <p>In radiation oncology, this evolution mirrors the broader field, with early applications focusing on rule-based planning systems, followed by statistical models for outcome prediction, and now deep learning approaches for tasks ranging from image segmentation to treatment planning optimization.</p> <h2 id="types-of-machine-learning">Types of Machine Learning</h2> <p>Machine learning, a subset of AI, focuses on developing algorithms that can learn patterns from data without being explicitly programmed. The field is typically divided into several learning paradigms:</p> <h3 id="supervised-learning">Supervised Learning</h3> <p>Supervised learning involves training models on labeled data, where each input is paired with the desired output. The algorithm learns to map inputs to outputs by minimizing the difference between its predictions and the ground truth labels. This approach is particularly relevant in radiation oncology for tasks like tumor classification, where historical images with confirmed diagnoses serve as training data.</p> <p>In supervised learning, the quality and quantity of labeled data are crucial factors in model performance. For medical applications, obtaining high-quality labeled data often requires expert annotation, which can be time-consuming and expensive. This challenge is particularly acute in radiation oncology, where inter-observer variability in contouring can introduce inconsistencies in training data.</p> <p>Common supervised learning tasks include classification (assigning inputs to discrete categories) and regression (predicting continuous values). In radiation oncology, classification might involve determining whether tissue is cancerous, while regression could predict radiation dose distribution.</p> <h3 id="unsupervised-learning">Unsupervised Learning</h3> <p>Unsupervised learning works with unlabeled data, seeking to discover inherent patterns or structures. Without explicit guidance on what constitutes a “correct” output, these algorithms identify natural groupings, reduce dimensionality, or detect anomalies in data.</p> <p>Clustering algorithms, a major category of unsupervised learning, group similar data points together based on distance metrics. In radiation oncology, clustering might be used to identify patient subgroups with similar treatment responses or to detect patterns in treatment planning that correlate with outcomes.</p> <p>Dimensionality reduction techniques like Principal Component Analysis (PCA) or t-SNE help visualize high-dimensional data and can reveal underlying structures not immediately apparent in the original feature space. These techniques can be valuable for analyzing the complex, multi-dimensional data generated in radiation treatment planning.</p> <p>Anomaly detection algorithms identify outliers or unusual patterns in data, which could represent equipment malfunctions, unusual patient anatomy, or potential errors in treatment plans.</p> <h3 id="reinforcement-learning">Reinforcement Learning</h3> <p>Reinforcement learning involves training agents to make sequences of decisions by rewarding desired behaviors and penalizing undesired ones. Unlike supervised learning, there are no labeled examples; instead, the agent learns through trial and error, guided by a reward signal.</p> <p>This paradigm has shown promise in treatment planning optimization, where the algorithm can learn to generate plans that maximize tumor coverage while minimizing dose to organs at risk. The reward function in such applications might incorporate clinical objectives like dose constraints and target coverage metrics.</p> <p>Reinforcement learning faces challenges in medical applications due to the need for extensive exploration (trying different actions to learn their outcomes), which may not be feasible in clinical settings where patient safety is paramount. Simulation environments and digital twins offer potential solutions, allowing algorithms to learn in virtual environments before deployment in clinical practice.</p> <h3 id="semi-supervised-and-self-supervised-learning">Semi-supervised and Self-supervised Learning</h3> <p>Semi-supervised learning combines elements of supervised and unsupervised learning, using a small amount of labeled data alongside a larger pool of unlabeled data. This approach is particularly relevant in medical imaging, where expert annotations may be limited but unannotated images are abundant.</p> <p>Self-supervised learning, a growing area of research, involves creating supervised learning tasks from unlabeled data by generating labels automatically. For example, an algorithm might be trained to predict missing portions of an image, with the complete image serving as the ground truth. These approaches show promise for pre-training models when labeled data is scarce.</p> <h2 id="key-machine-learning-algorithms">Key Machine Learning Algorithms</h2> <p>Before diving into deep learning, it’s essential to understand the traditional machine learning algorithms that form the foundation of the field. These algorithms continue to be valuable tools, especially when data is limited or interpretability is crucial.</p> <h3 id="linear-and-logistic-regression">Linear and Logistic Regression</h3> <p>Linear regression, one of the simplest machine learning algorithms, models the relationship between input features and a continuous output variable as a linear function. Despite its simplicity, linear regression provides a foundation for understanding more complex models and can be surprisingly effective for certain problems.</p> <p>Logistic regression extends this concept to classification problems by applying a sigmoid function to the linear output, transforming it into a probability between 0 and 1. This algorithm has been used in radiation oncology for predicting binary outcomes like tumor control or the development of specific toxicities.</p> <p>Both linear and logistic regression offer high interpretability, as the contribution of each feature to the prediction is explicitly represented by its coefficient. This transparency is valuable in clinical settings where understanding the basis for predictions is essential.</p> <h3 id="decision-trees-and-random-forests">Decision Trees and Random Forests</h3> <p>Decision trees partition the feature space through a series of binary splits, creating a tree-like structure where each leaf node represents a prediction. These models are intuitive and can capture non-linear relationships, but individual trees are prone to overfitting.</p> <p>Random forests address this limitation by combining many decision trees trained on different subsets of the data and features. The ensemble prediction is typically more robust and accurate than any individual tree. In radiation oncology, random forests have been used for tasks like predicting patient outcomes based on clinical factors, dosimetric parameters, and imaging features.</p> <p>Tree-based methods offer several advantages for medical applications, including handling mixed data types, robustness to outliers, and the ability to capture complex interactions between features. They also provide measures of feature importance, helping identify the most relevant factors for a given prediction task.</p> <h3 id="support-vector-machines">Support Vector Machines</h3> <p>Support Vector Machines (SVMs) find the optimal hyperplane that separates different classes in the feature space, maximizing the margin between the closest points (support vectors) from each class. Through the use of kernel functions, SVMs can efficiently handle non-linear decision boundaries.</p> <p>SVMs have been applied in radiation oncology for tasks like classifying treatment outcomes based on dosimetric and clinical features. Their ability to work well with relatively small datasets makes them suitable for many medical applications where data may be limited.</p> <h3 id="k-means-clustering">K-means Clustering</h3> <p>K-means clustering, an unsupervised learning algorithm, partitions data into K clusters by iteratively assigning points to the nearest cluster center and then updating those centers. This algorithm can identify natural groupings in patient data, potentially revealing subpopulations with distinct characteristics or treatment responses.</p> <p>In radiation oncology, k-means clustering has been used to identify patient subgroups based on anatomical features, dose distributions, or treatment outcomes. These insights can inform personalized treatment approaches and help identify patients who might benefit from alternative strategies.</p> <h2 id="feature-engineering-and-selection">Feature Engineering and Selection</h2> <p>Feature engineering—the process of creating, transforming, and selecting relevant features from raw data—plays a crucial role in the success of traditional machine learning algorithms. While deep learning can automatically learn useful representations from raw data, feature engineering remains important for many applications, especially when working with structured data or when interpretability is a priority.</p> <p>Common feature engineering techniques include:</p> <ol> <li> <p><strong>Normalization and standardization</strong>: Scaling features to a common range or distribution to prevent certain features from dominating the learning process due to their magnitude.</p> </li> <li> <p><strong>Polynomial features</strong>: Creating interaction terms between existing features to capture non-linear relationships.</p> </li> <li> <p><strong>Discretization</strong>: Converting continuous variables into categorical ones, which can sometimes reveal patterns not apparent in the continuous representation.</p> </li> <li> <p><strong>Text and image processing</strong>: Extracting meaningful features from unstructured data like medical reports or images.</p> </li> </ol> <p>Feature selection helps identify the most informative features while reducing dimensionality, which can improve model performance, reduce overfitting, and enhance interpretability. Methods include filter approaches (selecting features based on statistical measures), wrapper methods (evaluating feature subsets based on model performance), and embedded methods (incorporating feature selection into the model training process).</p> <p>In radiation oncology, domain knowledge plays a vital role in feature engineering. Clinically relevant features might include dosimetric parameters (like V20 for lung or mean heart dose), anatomical measurements, or derived metrics that capture aspects of the dose distribution known to correlate with outcomes.</p> <h2 id="model-evaluation-basics">Model Evaluation Basics</h2> <p>Proper evaluation is essential for understanding model performance and making informed decisions about deployment. Several key concepts underpin effective model evaluation:</p> <h3 id="train-test-splits-and-cross-validation">Train-Test Splits and Cross-Validation</h3> <p>Splitting data into training and testing sets allows evaluation on unseen data, providing a more realistic assessment of how the model will perform in practice. Cross-validation extends this concept by performing multiple train-test splits and averaging the results, providing a more robust performance estimate.</p> <p>In medical applications with limited data, techniques like stratified k-fold cross-validation help ensure that important subgroups are represented in both training and testing sets. Leave-one-out cross-validation, where each sample serves as the test set once, can be appropriate for very small datasets.</p> <h3 id="overfitting-and-underfitting">Overfitting and Underfitting</h3> <p>Overfitting occurs when a model learns the training data too well, capturing noise rather than the underlying pattern. This results in poor generalization to new data. Underfitting, conversely, happens when a model is too simple to capture the underlying pattern.</p> <p>Regularization techniques, early stopping, and proper validation can help detect and mitigate overfitting. In radiation oncology, where datasets are often small, the risk of overfitting is particularly high, making these techniques especially important.</p> <h3 id="performance-metrics">Performance Metrics</h3> <p>Different metrics capture different aspects of model performance:</p> <ul> <li><strong>Accuracy</strong>: The proportion of correct predictions, useful for balanced classification problems.</li> <li><strong>Precision and Recall</strong>: Important for imbalanced problems, precision measures the proportion of positive predictions that are correct, while recall measures the proportion of actual positives that are correctly identified.</li> <li><strong>F1 Score</strong>: The harmonic mean of precision and recall, providing a balance between the two.</li> <li><strong>Area Under the ROC Curve (AUC)</strong>: Measures the model’s ability to discriminate between classes across different threshold settings.</li> <li><strong>Mean Squared Error (MSE) and Mean Absolute Error (MAE)</strong>: Common metrics for regression problems.</li> </ul> <p>In radiation oncology, domain-specific metrics are often more relevant than generic ones. For contouring tasks, metrics like Dice similarity coefficient and Hausdorff distance measure the overlap and maximum distance between predicted and ground truth contours. For treatment planning, metrics might include target coverage, conformity indices, and dose to organs at risk.</p> <h2 id="limitations-of-traditional-machine-learning">Limitations of Traditional Machine Learning</h2> <p>While traditional machine learning algorithms have proven valuable in many applications, they have several limitations that deep learning addresses:</p> <ol> <li> <p><strong>Feature engineering dependency</strong>: Traditional algorithms rely heavily on manual feature engineering, which requires domain expertise and can miss complex patterns that aren’t explicitly encoded.</p> </li> <li> <p><strong>Difficulty with unstructured data</strong>: Images, text, and other unstructured data types are challenging for traditional algorithms without extensive preprocessing.</p> </li> <li> <p><strong>Limited representation capacity</strong>: Many traditional algorithms struggle to capture complex, hierarchical patterns in data.</p> </li> <li> <p><strong>Fixed model complexity</strong>: The complexity of traditional models is often fixed or limited by design, constraining their ability to scale with data size.</p> </li> <li> <p><strong>Separate learning stages</strong>: Traditional pipelines often involve separate stages for feature extraction and model training, preventing end-to-end optimization.</p> </li> </ol> <p>Deep learning addresses these limitations through its ability to automatically learn hierarchical representations from raw data, scale with data and computational resources, and enable end-to-end training. However, traditional methods retain advantages in scenarios with limited data, when interpretability is crucial, or when computational resources are constrained.</p> <p>Understanding these traditional approaches provides important context for appreciating the innovations and capabilities of deep learning, which we’ll explore in subsequent modules. It also helps identify situations where simpler models might be more appropriate than complex deep learning architectures.</p> <h3 id="further-reading">Further Reading</h3> <p>“Artificial intelligence in radiation oncology”: <a href="https://pubmed.ncbi.nlm.nih.gov/32843739/">This article </a> provides a comprehensive overview of AI methods and their implications in the radiation therapy process. ​</p> <p>“Revolutionizing radiation therapy: the role of AI in clinical practice”: <a href="https://academic.oup.com/jrr/article/65/1/1/7441099">This article</a> discusses how AI has optimized tumor and organ segmentation, saving time for radiation oncologists. ​</p> <p>“Artificial intelligence in radiation oncology: A review of its current applications and future outlook”: <a href="https://www.sciencedirect.com/science/article/pii/S1078817421000924">This review</a> explores the potential of AI in toxicity prediction, automated treatment planning, and clinical trial patient selection.</p> <p>“Artificial intelligence and machine learning in cancer imaging”: <a href="https://www.nature.com/articles/s43856-022-00199-0">This article</a> discusses the challenges and opportunities of AI and ML in cancer imaging, considerations for the development of algorithms into tools that can be widely used and disseminated, and the development of the ecosystem needed to promote growth of AI and ML in cancer imaging.</p>]]></content><author><name></name></author><category term="tairo,"/><category term="radiation-oncology,"/><category term="artificial-intelligence"/><summary type="html"><![CDATA[1: Introduction]]></summary></entry><entry><title type="html">Paper Summary: Deep learning in medical imaging and radiation therapy</title><link href="https://amithjkamath.github.io/blog/2024/deep-learning-in-radiation-therapy/" rel="alternate" type="text/html" title="Paper Summary: Deep learning in medical imaging and radiation therapy"/><published>2024-01-01T00:00:00+00:00</published><updated>2024-01-01T00:00:00+00:00</updated><id>https://amithjkamath.github.io/blog/2024/deep-learning-in-radiation-therapy</id><content type="html" xml:base="https://amithjkamath.github.io/blog/2024/deep-learning-in-radiation-therapy/"><![CDATA[<h2 id="background-and-introduction">Background and Introduction</h2> <p>The success of DL compared to traditional machine learning methods is primarily based on two interrelated factors: depth and compositionality. A function is said to have a compact expression if it has few computational elements used to represent it (“few” here is a relative term that depends on the complexity of the function). An architecture with sufficient depth can produce a compact representation, whereas an insufficiently deep one may require an exponentially larger architecture (in terms of the number of computational elements that need to be learned) to represent the same function.</p> <p>A compact representation requires fewer training examples to tune the parameters and produces better generalization to unseen examples. This is critically important in complex tasks such as computer vision where each object class can exhibit many variations in appearance which would potentially require several examples per type of variation in the training set if a compact representation is not used.</p> <p>The second advantage of deep architectures has to do with how successive layers of the network can utilize the representations from previous layers to compose more complex representations that better capture critical characteristics of the input data and suppress the irrelevant variations (for instance, simple translations of an object in the image should result in the same classification). In image recognition, deep networks have been shown to capture simple information such as the presence or absence of edges at different locations and orientations in the first layer. Successive layers of the network assemble the edges into compound edges and corners of shapes, and then into more and more complex shapes that resemble object parts.</p> <p>Hierarchical representation learning is very useful in complicated tasks such as computer vision where adjacent pixels and object parts are correlated with each other and their relative locations provide clues about each class of object, or speech recognition and natural language processing where the sequence of words follow contextual and grammatical rules that can be learned from the data.</p> <h2 id="convolutional-neural-networks">Convolutional Neural Networks</h2> <p>The most successful and popular DL architecture in imaging is the convolutional neural network (CNN). Nearby pixels in an image are correlated with one another both in areas that exhibit local smoothness and areas consisting of structures (e.g., edges of objects or textured regions). These correlations typically manifest themselves in different parts of the same image. Accordingly, instead of having a fully connected network where every pixel is processed by a different weight, every location can be processed using the same set of weights to extract various repeating patterns across the entire image. These sets of trainable weights, referred to as kernels or filters, are applied to the image using a dot product or convolution and then processed by a nonlinearity (e.g., a sigmoid or tanh function). Each of these convolution layers can consist of many such filters resulting in the extraction of multiple sets of patterns at each layer.</p> <h2 id="as-applied-to-medical-imaging">As applied to Medical Imaging</h2> <p>In medical imaging, machine learning algorithms have been used for decades, starting with algorithms to analyze or help interpret radiographic images in the mid-1960s. Computer-aided detection/diagnosis (CAD) algorithms started to make advances in the mid 1980s, first with algorithms dedicated to cancer detection and diagnosis on chest radiographs and mammograms and then widening in scope to other modalities such as computed tomography (CT) and ultrasound. CAD algorithms in the early days predominantly used a data-driven approach as most DL algorithms do today. However, unlike most DL algorithms, most of these early CAD methods heavily depended on feature engineering.</p> <p>DL for radiological images, and shows a very strong trend: For example, in the first 3 months of 2018, more papers were published on this topic than the whole year of 2016.</p> <h2 id="image-segmentation-with-deep-learning">Image Segmentation with Deep Learning</h2> <p>Image segmentation in medical imaging based on DL generally uses two different input methods: (a) patches of an input image and (b) the entire image. Both methods generate an output map that provides the likelihood that a given region is part of the object being segmented. While patch-based segmentation methods were initially used, most recent studies use the entire input image to give contextual information and reduce redundant calculations.</p> <p>Lesion segmentation is a similar task to organ segmentation; however, lesion segmentation is generally more difficult than organ segmentation, as the object being segmented can have varying shapes and sizes.</p> <h2 id="dl-and-radiotherapy">DL and Radiotherapy</h2> <p>The goals of DL in radiation oncology are to assist in treatment planning, assess response to therapy, and provide automated adaptation in treatments over time. Deep reinforcement learning using both prior treatment plans and methods for assessing tumor local control was used to automatically estimate dose protocols. Such adaptive radiotherapy methods may provide clinical decision support for dose adaptation.</p> <p>Much of the needs in treatment planning relate to the segmentation of organs (discussed earlier) and in the prediction of dose distributions from contours. Nguyen et al used a U-net to predict dose from patient image contours on prostate intensity-modulated radiation therapy (IMRT) patients and demonstrated desired radiation dose distributions.</p> <p>While DL methods are being developed to plan and predict radiation therapy to specific tumor sites, they are also being investigated to assess toxicity to normal organs tissue.</p> <h2 id="more-to-read-from-here">More to read from here:</h2> <p>281: Zhen et al. used a transfer learning strategy to predict rectum dose toxicity for cervical cancer radiotherapy.</p> <p>283: Dose estimation was also the aim of Kajikawa et al. who investigated the feasibility of DL in the automated determination of dosimetric eligibility of prostate cancer patients undergoing intensity-modulated radiation therapy.</p> <p>218: Lao et al. investigated MRI radiomic features and DL as a means to predict survival in glioblastoma multiforme.</p> <h2 id="challenges-for-deep-learning-methods">Challenges for Deep Learning methods</h2> <p>Robustness is a challenge: Robustness and repeatability are concerns with any machine learning approach,365 and even more so with DL. Since medical image datasets are so difficult to come by compared to those of natural images and generally are of limited size, researchers like to reuse the same data for different tasks. Hence, correction for multiple comparisons is crucial in the statistical evaluation of performance. The requirement that datasets need to be of sufficient size and quality is not unique to DL or medical imaging.</p> <h2 id="references">References</h2> <p>Sahiner, B., Pezeshk, A., Hadjiiski, L. M., Wang, X., Drukker, K., Cha, K. H., Summers, R. M., &amp; Giger, M. L. (2019). Deep learning in medical imaging and radiation therapy. Medical Physics, 46(1), e1–e36.</p> <p><a href="https://doi.org/10.1002/mp.13264">Paper link on DOI</a></p>]]></content><author><name></name></author><category term="paper-summary,"/><category term="computer-vision,"/><category term="mia"/><summary type="html"><![CDATA[Background and Introduction]]></summary></entry><entry><title type="html">Paper Summary: Quality Assurance for AI-Based Applications in Radiation Therapy</title><link href="https://amithjkamath.github.io/blog/2024/qa-for-ai-in-radiotherapy/" rel="alternate" type="text/html" title="Paper Summary: Quality Assurance for AI-Based Applications in Radiation Therapy"/><published>2024-01-01T00:00:00+00:00</published><updated>2024-01-01T00:00:00+00:00</updated><id>https://amithjkamath.github.io/blog/2024/qa-for-ai-in-radiotherapy</id><content type="html" xml:base="https://amithjkamath.github.io/blog/2024/qa-for-ai-in-radiotherapy/"><![CDATA[<p>AI algorithms are typically data-driven, may be continuously evolving, and their behavior has a degree of (acceptable) uncertainty due to inherent noise in training data and the substantial number of parameters that are used in the algorithms.</p> <p>QA for AI-based systems is an emerging area, which has not been intensively explored and requires interactive collaborations between medical doctors, medical physics experts, and commercial/research AI institutions.</p> <p>Given their unique role as a bridge between the clinical environment and new technologies, medical physics experts are most likely the main frontiers to implementing these automated systems to improve efﬁciency, quality, standardization, and acceleration of the workﬂow leading to more safe and accurate radiation administration.</p> <p>It is, therefore, crucial to provide clear guidance for understanding and tackling the difﬁculties inherent in high-quality AI systems. This has been recognized by different societies in medical physics, which have recently published a detailed Checklist for AI in Medical Physics (CLAMP).</p> <p>Case-speciﬁc QA refers to all checks that verify the output of an AI-based application generated for each patient or machine.10 When QA results are satisfactory, the output can be used in the RT workﬂow. When the case-speciﬁc QA check fails, the output is subject to a second veriﬁcation. Depending on the application, the quality of the model’s output can be monitored in multiple ways. Currently, human supervision of the output, in combination with quantitative/qualitative measures, is seen as one of the most important tools. Automatic case-speciﬁc QA methods can be utilized to highlight divergent behavior. When too frequent failures are detected during case-specific QA, a routine QA is carried out to determine whether a recommissioning of the deployed model is needed.</p> <p>Routine QA is dedicated to the regular supervision of the AI model validity with the intention to monitor if the model’s output changes unexpectedly after clinical workﬂow changes (eg, software update, etc.). For this purpose, a periodical end-to-end performance should be completed on a reference test dataset. When routine QA tests do not meet expectations, a model re-commissioning may be necessary.</p> <h2 id="references">References</h2> <p>Claessens, M., Oria, C. S., Brouwer, C. L., Ziemer, B. P., Scholey, J. E., Lin, H., Witztum, A., Morin, O., Naqa, I. el, van Elmpt, W., &amp; Verellen, D. (2022). Quality Assurance for AI-Based Applications in Radiation Therapy. In Seminars in Radiation Oncology (Vol. 32, Issue 4, pp. 421–431). W.B. Saunders.</p> <p><a href="https://doi.org/10.1016/j.semradonc.2022.06.011">Paper link on DOI</a></p>]]></content><author><name></name></author><category term="paper-summary,"/><category term="computer-vision,"/><category term="mia"/><summary type="html"><![CDATA[AI algorithms are typically data-driven, may be continuously evolving, and their behavior has a degree of (acceptable) uncertainty due to inherent noise in training data and the substantial number of parameters that are used in the algorithms.]]></summary></entry><entry><title type="html">About Us: “I would like to convert my research into a useful tool for clinicians.” - Center for Artificial Intelligence in Medicine (CAIM)</title><link href="https://amithjkamath.github.io/blog/2022/about-us-i-would-like-to-convert-my-research-into-a-useful-tool-for-clinicians-center-for-artificial-intelligence-in-medicine-caim/" rel="alternate" type="text/html" title="About Us: “I would like to convert my research into a useful tool for clinicians.” - Center for Artificial Intelligence in Medicine (CAIM)"/><published>2022-12-01T00:00:00+00:00</published><updated>2022-12-01T00:00:00+00:00</updated><id>https://amithjkamath.github.io/blog/2022/about-us-i-would-like-to-convert-my-research-into-a-useful-tool-for-clinicians---center-for-artificial-intelligence-in-medicine-caim</id><content type="html" xml:base="https://amithjkamath.github.io/blog/2022/about-us-i-would-like-to-convert-my-research-into-a-useful-tool-for-clinicians-center-for-artificial-intelligence-in-medicine-caim/"><![CDATA[<p>December 2022Amith Kamath wishes to facilitate faster radiotherapy treatment for patients with glioblastoma through AI-supported therapy planning. The CAIM Young Researcher Award winner appreciates the openness of the Bernese community around AI applications in healthcare, also welcoming ideas from people trained in other disciplines to tackle hard problems in medicine. He is currently pursuing his PhD at the Medical Image Analysis research group of the ARTORG Center for Biomedical Engineering Research and looks forward to translating his research into a clinical tool through the broad entrepreneurial support he is receiving in Bern – including the personalized business coaching by be-advanced as part of his CAIM Award win in the category “translation”.What drives you in your research? My research is centered around evaluating the quality of radiotherapy delivered to patients with glioblastoma. Given the usually bad prognosis, people already diagnosed with this tumor currently must wait between one and three weeks until they can start treatment, due to the current workflows in radiotherapy planning. We expect that by using AI models to help draw boundaries around organs while simultaneously estimating the radiation dose and toxicity, radiotherapy treatment can be started earlier before the tumor has progressed further. We hope that someday our work can really add quality to people’s lives in this sense.The challenge in radiotherapy for glioblastoma is to be targeted while killing the tumor but sparing healthy areas of the brain. Mistakes made in these initial steps in the process can add up in subsequent steps, underscoring the importance of being precise. For example, if you irradiate healthy tissue in the brain, people can lose their functional abilities, for example speech or motor abilities. Our idea is to use deep neural networks to not only estimate, but also simulate inter-expert variations in boundaries that are drawn around tumors as well as healthy areas during radiotherapy planning. These simulations give us a better sense of the range of safe variations in how human experts manually do this and thereby understand the clinical impact of such variations in the process, leading to safer treatment.What does winning a CAIM Young Researcher Award mean to you? What matters to me most is that many of us were able to share our research and receive constructive feedback and comments in a setting like the CAIM Symposium. Beyond the award, the existence of such a vibrant community is very rewarding. For the translational focus of the award, I was fortunate to receive prior exposure through the Innosuisse startup toolbox program “Business Concepts” in October this year. The entrepreneurial coaching opportunity I now have with the CAIM Award is the perfect continuation of that. It would be great to get the experts’ opinion on how we can convert our research into a useful tool or product for clinicians!The unique thing here in Bern is the entrepreneurial support you receive, for example through the Swiss Institute for Translational and Entrepreneurial Medicine, sitem-insel. There is a lot of existing knowledge amongst the faculty about how a PhD project can be shaped into a product that can be used in clinics, which is quite exciting! If there are ten users of what I build, that will mean more to me than writing a long PhD thesis that no one may read.How important is it for you to share your research? Very much! My background is mostly in image processing and computer vision, and I think it’s great how welcoming the scientific community here is to researchers from other academic backgrounds. I don’t have a biomedicine background, and I believe people without medical schooling can make strong contributions to tackling hard problems in the medical space. Ways of thinking that are commonplace in another field could be novel to healthcare challenges and thus lead to innovative solutions. I like the people I get to work with daily who motivate me by asking all the right questions. I like that my work is very visual: I find it easier to look at a set of images or a video than at a bunch of equations for an “Aha” moment. When some images are hard to interpret, I appreciate that clinicians are quite open to talk to technical folks like us. This readiness to work with each other and speak the same language is quite important in this line of work. Therefore, I like to share our research with a broader global community. I use Social Media to exchange ideas with other scientists and our research lab Medical Image Analysis has started a “How to” video series for beginners in Deep Learning for medical imaging, summarizing some of the pitfalls and stumbling blocks in a humorous way (https://github.com/ubern-mia/bender). We are also currently preparing for a symposium on interpretability of AI models at CAIM in March ‘23, with the hope to get a lively discussion going on this important topic for safer AI adoption in medicine.Amith is a computer scientist and holds a Master of Science in Computer Science from Georgia Institute of Technology, in the US. He worked earlier as a software developer at the MathWorks Inc., on the Image Processing and Computer Vision Toolboxes in MATLAB, a scientific computing programming language. Prior to that, he earned a Master of Science in Electrical Engineering at University of Minnesota, and a Bachelor of Technology from National Institute of Technology Karnataka, in Surathkal, India. Currently, he is pursuing a PhD in Biomedical Engineering at the ARTORG Center at the University of Bern, under the supervision of Prof. Dr. Mauricio Reyes.His PhD thesis is on image segmentation and how one could use AI models to not only automate the otherwise time and effort intensive segmentation process, but further evaluate the quality of the contours in comparison to human-experts. His research focuses both on the robustness of using AI models to perform auto-segmentation, as well as computing radiotherapy dose predictions from these contours faster than current methods used in clinical practice. These results could help improve the speed as well as the quality and safety of radiotherapy plans for patients suffering from glioblastoma.Bern Interpretable AI Symposium (BIAS): www.caim.unibe.ch/bias2023</p>]]></content><author><name></name></author></entry><entry><title type="html">Paper Summary: Which Explanation Should I Choose? A Function Approximation Perspective to Characterizing Post hoc Explanations</title><link href="https://amithjkamath.github.io/blog/2022/Which-explanation-should-I-choose/" rel="alternate" type="text/html" title="Paper Summary: Which Explanation Should I Choose? A Function Approximation Perspective to Characterizing Post hoc Explanations"/><published>2022-11-16T00:00:00+00:00</published><updated>2022-11-16T00:00:00+00:00</updated><id>https://amithjkamath.github.io/blog/2022/Which-explanation-should-I-choose</id><content type="html" xml:base="https://amithjkamath.github.io/blog/2022/Which-explanation-should-I-choose/"><![CDATA[<p>This paper …</p> <p>Major contributions of this work include: -</p> <h1 id="major-learning-points">Major Learning Points</h1> <ol> <li></li> <li></li> </ol> <h1 id="interesting-bits">Interesting bits</h1> <ol> <li></li> <li></li> </ol> <h2 id="references">References</h2> <p><a href="https://arxiv.org/abs/2206.01254">Paper link on Arxiv</a></p> <p>Paper code doesn’t appear to be released yet, although is mentioned in the appendix.</p>]]></content><author><name></name></author><category term="paper-summary,"/><category term="computer-vision,"/><category term="mia"/><summary type="html"><![CDATA[This paper …]]></summary></entry><entry><title type="html">Paper Summary: Volumetric memory network for interactive medical image segmentation</title><link href="https://amithjkamath.github.io/blog/2022/Volumetric-memory-networks/" rel="alternate" type="text/html" title="Paper Summary: Volumetric memory network for interactive medical image segmentation"/><published>2022-11-03T00:00:00+00:00</published><updated>2022-11-03T00:00:00+00:00</updated><id>https://amithjkamath.github.io/blog/2022/Volumetric-memory-networks</id><content type="html" xml:base="https://amithjkamath.github.io/blog/2022/Volumetric-memory-networks/"><![CDATA[<p>This paper …</p> <p>Major contributions of this work include: -</p> <h1 id="major-learning-points">Major Learning Points</h1> <ol> <li></li> <li></li> </ol> <h1 id="interesting-bits">Interesting bits</h1> <ol> <li></li> <li></li> </ol> <h2 id="references">References</h2> <p><a href="https://www.sciencedirect.com/science/article/pii/S1361841522002316">Paper link on Sciencedirect</a></p> <p><a href="https://github.com/lingorX/Mem3D">Paper code</a></p>]]></content><author><name></name></author><category term="paper-summary,"/><category term="computer-vision,"/><category term="mia"/><summary type="html"><![CDATA[This paper …]]></summary></entry></feed>